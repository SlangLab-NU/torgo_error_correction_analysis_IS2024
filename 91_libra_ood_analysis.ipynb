{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f8e4d83-4570-422d-851b-cb99e59ab273",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2dbde2-508c-4d39-98ab-3cdb67d060b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speakers = ['MC01', 'MC02', 'MC03', 'MC04','FC01','FC02','FC03']\n",
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44ab2c-d401-4e43-8718-6a46506d6758",
   "metadata": {},
   "source": [
    "# read libra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "452b64a6-d64d-49d1-a476-0f4d1cf89824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30f8514ad0424536ad7fba7afde25414",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/11.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51761e05aa5f49be9f1576dd6cf34a48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/10.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a2bcd7a4b7446881206eacedddf579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b347ae2f704d5ba8458c5e8c1711b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/347M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70dca0b248f94e918a7b972968e7b9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.39G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "560e2bf6a9a44a658e307d9b50075221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/23.0G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d74fdbb31cb4bbda5d7777d8ba9ec82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train.100 split:   0%|          | 0/28539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9f84e4a0fe4bbba090fec95c31b93c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train.360 split:   0%|          | 0/104014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bcb5311123445e8a19c0dc03f9e31b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/2703 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95901886bc94b578c5a7d145e830d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/2620 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96205ee8548b4697a927ad4707e4e462",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "libra = load_dataset(\"librispeech_asr\", 'clean', cache_dir=\"/work/van-speech-nlp/cache\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85286549-92de-4adf-86ca-fba1d64638ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['THE SECOND IN IMPORTANCE IS AS FOLLOWS SOVEREIGNTY MAY BE DEFINED TO BE THE RIGHT OF MAKING LAWS IN FRANCE THE KING REALLY EXERCISES A PORTION OF THE SOVEREIGN POWER SINCE THE LAWS HAVE NO WEIGHT',\n",
       " 'TILL HE HAS GIVEN HIS ASSENT TO THEM HE IS MOREOVER THE EXECUTOR OF ALL THEY ORDAIN THE PRESIDENT IS ALSO THE EXECUTOR OF THE LAWS BUT HE DOES NOT REALLY CO OPERATE IN THEIR FORMATION',\n",
       " 'SINCE THE REFUSAL OF HIS ASSENT DOES NOT ANNUL THEM HE IS THEREFORE MERELY TO BE CONSIDERED AS THE AGENT OF THE SOVEREIGN POWER BUT NOT ONLY DOES THE KING OF FRANCE EXERCISE A PORTION OF THE SOVEREIGN POWER']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations=libra['train.360']['text']\n",
    "translations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30256055-d96c-4a1c-af26-222ccec0127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_translations = []\n",
    "\n",
    "for text in translations:\n",
    "    cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "    cleaned_translations.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "deab9125-7c88-43a6-b37a-092b69e710cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the second in importance is as follows sovereignty may be defined to be the right of making laws in france the king really exercises a portion of the sovereign power since the laws have no weight',\n",
       " 'till he has given his assent to them he is moreover the executor of all they ordain the president is also the executor of the laws but he does not really co operate in their formation',\n",
       " 'since the refusal of his assent does not annul them he is therefore merely to be considered as the agent of the sovereign power but not only does the king of france exercise a portion of the sovereign power']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_translations[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bef9b791-2ef9-4a02-a403-584159103b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"DATASET_libri.txt\", \"w\") as file:\n",
    "    for text in cleaned_translations:\n",
    "        file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "341bbe8c-a2c7-4807-b377-c1e3fc0c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set_europarl = set()\n",
    "\n",
    "for sentence in cleaned_translations:\n",
    "    words = sentence.split()\n",
    "    word_set_europarl.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05212f13-66ee-41da-8c4a-56be8561552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58354"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set_europarl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0871fd9-e59c-4713-8eda-c8ed2483794a",
   "metadata": {},
   "source": [
    "# train LM with LIBRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8038b79e-78e5-4c2d-a3a2-66bc32b0239e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/DATASET_libri.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 3595494 types 58357\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:700284 2:55979069440 3:104960753664\n",
      "Statistics:\n",
      "1 58357 D1=0.578511 D2=0.988481 D3+=1.43628\n",
      "2 986830 D1=0.758182 D2=1.0831 D3+=1.37103\n",
      "3 2458575 D1=0.857684 D2=1.17924 D3+=1.35938\n",
      "Memory estimate for binary LM:\n",
      "type    MB\n",
      "probing 66 assuming -p 1.5\n",
      "probing 72 assuming -r models -p 1.5\n",
      "trie    26 without quantization\n",
      "trie    14 assuming -q 8 -b 8 quantization \n",
      "trie    25 assuming -a 22 array pointer compression\n",
      "trie    13 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:700284 2:15789280 3:49171500\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:700284 2:15789280 3:49171500\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:157323252 kB\tVmRSS:20836 kB\tRSSMax:36339912 kB\tuser:7.22229\tsys:22.5113\tCPU:29.7336\treal:29.4939\n"
     ]
    }
   ],
   "source": [
    "!kenlm/build/bin/lmplz -o 3 < DATASET_libri.txt > libri.apra --discount_fallback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df452-e4a2-4910-95e1-eb8f1768952a",
   "metadata": {},
   "source": [
    "# read torgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26792180-b682-4a11-ab3a-93ea53cead51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo.csv')\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bee5d29-8727-40a1-99fc-f2ac4b58bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['session', 'audio', 'text', 'speaker_id'],\n",
       "        num_rows: 16394\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cf3bf24b-028d-43d4-84b1-bf5caa6a7c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = dataset_csv['train']['text']\n",
    "\n",
    "cleaned_texts = []\n",
    "for text in all_texts:\n",
    "    cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "    cleaned_texts.append(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "463bb5c6-717f-46b5-b269-652fe5a28e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stick',\n",
       " 'except in the winter when the ooze or snow or ice prevents',\n",
       " 'pat',\n",
       " 'up',\n",
       " 'meat',\n",
       " 'meat',\n",
       " 'know',\n",
       " 'he slowly takes a short walk in the open air each day',\n",
       " 'air',\n",
       " 'swarm']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "32078aae-bf70-418a-b97f-4acff0ad16d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 228\n",
      "F03: 1075\n",
      "F04: 667\n",
      "M01: 739\n",
      "M02: 766\n",
      "M03: 800\n",
      "M04: 652\n",
      "M05: 573\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "speaker_texts = defaultdict(list)\n",
    "\n",
    "for i in range(len(data_df)):\n",
    "    speaker_id = data_df.loc[i, 'speaker_id']\n",
    "    # print(speaker_id)\n",
    "    if speaker_id not in normal_speakers:\n",
    "        text = data_df.loc[i, 'text']\n",
    "        speaker_texts[speaker_id].append(text)\n",
    "\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    print(f\"{speaker_id}: {len(texts)}\")\n",
    "    # for text in texts:\n",
    "    #     print(text)\n",
    "    # print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb2347-a2e7-4fff-8066-a9656bbcab0a",
   "metadata": {},
   "source": [
    "# calculate unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0a936ba-0bca-412f-b83e-20b2a10af5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 190 unique words\n",
      "F03: 768 unique words\n",
      "F04: 669 unique words\n",
      "M01: 605 unique words\n",
      "M02: 622 unique words\n",
      "M03: 645 unique words\n",
      "M04: 450 unique words\n",
      "M05: 718 unique words\n"
     ]
    }
   ],
   "source": [
    "unique_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    unique_words = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
    "            if cleaned_word:  \n",
    "                unique_words.add(cleaned_word) \n",
    "    unique_words_per_speaker[speaker_id] = unique_words\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    print(f\"{speaker_id}: {len(unique_words)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbe8f9-43db-4b41-bfda-e760ff39795f",
   "metadata": {},
   "source": [
    "# vocabulary similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "78b5d037-a6c3-4fc6-99ed-fe6e2c2630a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 0.53% dissimilarity with word_set_europarl\n",
      "F01: out_of_domain_words count: 1\n",
      "Out of domain words for F01: {'gadget'}\n",
      "\n",
      "F03: 2.21% dissimilarity with word_set_europarl\n",
      "F03: out_of_domain_words count: 17\n",
      "Out of domain words for F03: {'gadget', 'validated', 'ninetythree', 'whitecapped', 'shimmers', 'tort', 'mut', 'sews', 'cupping', 'advisement', 'foxtrot', 'snoop', 'tango', 'kilo', 'treehouse', 'sweaters', 'exam'}\n",
      "\n",
      "F04: 2.69% dissimilarity with word_set_europarl\n",
      "F04: out_of_domain_words count: 18\n",
      "Out of domain words for F04: {'gadget', 'xray', 'ninetythree', 'validated', 'shimmers', 'mut', 'lilyrare', 'sews', 'ticker', 'leaks', 'advisement', 'bloat', 'foxtrot', 'snoop', 'kilo', 'treehouse', 'sweaters', 'exam'}\n",
      "\n",
      "M01: 2.48% dissimilarity with word_set_europarl\n",
      "M01: out_of_domain_words count: 15\n",
      "Out of domain words for M01: {'gadget', 'validated', 'ninetythree', 'shimmers', 'mut', 'sews', 'ticker', 'advisement', 'bloat', 'foxtrot', 'snoop', 'kilo', 'treehouse', 'sweaters', 'exam'}\n",
      "\n",
      "M02: 2.41% dissimilarity with word_set_europarl\n",
      "M02: out_of_domain_words count: 15\n",
      "Out of domain words for M02: {'gadget', 'validated', 'ninetythree', 'shimmers', 'mut', 'sews', 'advisement', 'leaks', 'bloat', 'foxtrot', 'snoop', 'kilo', 'treehouse', 'sweaters', 'exam'}\n",
      "\n",
      "M03: 2.33% dissimilarity with word_set_europarl\n",
      "M03: out_of_domain_words count: 15\n",
      "Out of domain words for M03: {'gadget', 'validated', 'ninetythree', 'shimmers', 'mut', 'sews', 'advisement', 'bloat', 'foxtrot', 'snoop', 'tango', 'kilo', 'treehouse', 'sweaters', 'exam'}\n",
      "\n",
      "M04: 2.00% dissimilarity with word_set_europarl\n",
      "M04: out_of_domain_words count: 9\n",
      "Out of domain words for M04: {'gadget', 'validated', 'ninetythree', 'shimmers', 'mut', 'sews', 'snoop', 'treehouse', 'advisement'}\n",
      "\n",
      "M05: 2.92% dissimilarity with word_set_europarl\n",
      "M05: out_of_domain_words count: 21\n",
      "Out of domain words for M05: {'13th', 'whitecapped', 'ticker', 'leaks', 'marshmallows', 'bloat', 'tango', 'exam', 'validated', 'ninetythree', 'snoop', 'treehouse', 'sweaters', 'gadget', 'xray', 'shimmers', 'mut', 'sews', 'kilo', 'advisement', 'foxtrot'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_of_domain_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    overlap = len(unique_words & word_set_europarl)\n",
    "    similarity = overlap / len(unique_words) if len(unique_words) > 0 else 0\n",
    "    out_of_domain_similarity = (1 - similarity)*100\n",
    "\n",
    "    out_of_domain_words = {word for word in unique_words if word not in word_set_europarl}\n",
    "    out_of_domain_words_per_speaker[speaker_id] = out_of_domain_words\n",
    "\n",
    "    print(f\"{speaker_id}: {out_of_domain_similarity:.2f}% dissimilarity with word_set_europarl\")\n",
    "    print(f\"{speaker_id}: out_of_domain_words count: {len(out_of_domain_words)}\")\n",
    "    print(f\"Out of domain words for {speaker_id}: {out_of_domain_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e061463-697d-4da9-ba06-e060691eae8c",
   "metadata": {},
   "source": [
    "# Out of domain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ca79b25-8f46-4b56-a927-acef8b00a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10139.860730126507\n",
      "482.3399492347551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri.apra\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "model=kenlm.Model(\"libri.apra\") \n",
    "per=model.perplexity(\"your text sentance\")\n",
    "print(per)\n",
    "\n",
    "per=model.perplexity(\"resumption of the session\")\n",
    "print(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae7fba71-6e51-412e-acd9-24d3e255e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: 3979.8411851554615\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity for each text\n",
    "perplexities = []\n",
    "for text in cleaned_texts:\n",
    "    perplexity = model.perplexity(text)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Calculate the average perplexity\n",
    "avg_perplexity = np.mean(perplexities)\n",
    "print(f\"Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15858ed1-b922-41f0-948d-a2455abaccea",
   "metadata": {},
   "source": [
    "# clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce1fc610-f314-499d-891a-b150c07f3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01:\n",
      "stick\n",
      "except in the winter when the ooze or snow or ice prevents\n",
      "\n",
      "Speaker F03:\n",
      "beta\n",
      "stubble\n",
      "\n",
      "Speaker F04:\n",
      "fee\n",
      "yet he still thinks as swiftly as ever\n",
      "\n",
      "Speaker M01:\n",
      "when he speaks his voice is just a bit cracked and quivers a trifle\n",
      "trait\n",
      "\n",
      "Speaker M02:\n",
      "grow\n",
      "feed\n",
      "\n",
      "Speaker M03:\n",
      "now\n",
      "know\n",
      "\n",
      "Speaker M04:\n",
      "trouble\n",
      "spark\n",
      "\n",
      "Speaker M05:\n",
      "swore\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned_speaker_texts = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    cleaned_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    print(f\"Speaker {speaker_id}:\")\n",
    "    for text in texts[:2]:  \n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60dadda6-a29a-46f6-8613-e473e0278871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01: Average Perplexity: 3321.6634825919314\n",
      "Speaker F03: Average Perplexity: 3828.1590222953987\n",
      "Speaker F04: Average Perplexity: 4077.993261310603\n",
      "Speaker M01: Average Perplexity: 3917.8475900790772\n",
      "Speaker M02: Average Perplexity: 3862.7596868415703\n",
      "Speaker M03: Average Perplexity: 3824.284655645186\n",
      "Speaker M04: Average Perplexity: 3908.6444183261924\n",
      "Speaker M05: Average Perplexity: 4063.3047017015047\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store perplexities per speaker\n",
    "perplexities_per_speaker = {}\n",
    "\n",
    "# Calculate perplexity for each speaker\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    perplexities = []\n",
    "    for text in texts:\n",
    "        perplexity = model.perplexity(text)\n",
    "        perplexities.append(perplexity)\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    perplexities_per_speaker[speaker_id] = avg_perplexity\n",
    "\n",
    "# Print average perplexity for each speaker\n",
    "for speaker_id, avg_perplexity in perplexities_per_speaker.items():\n",
    "    print(f\"Speaker {speaker_id}: Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f47b11-e6b1-492a-bcc1-43f10ae4e304",
   "metadata": {},
   "source": [
    "# output unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7d644966-e731-4845-975f-3268b6806b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "arpa_file = f\"libri_lm_arpa_files/libri.apra\"\n",
    "output_file = f\"libri_lm_arpa_files/unigram.txt\"\n",
    "\n",
    "unigrams = []\n",
    "\n",
    "with open(arpa_file, \"r\") as file:\n",
    "    start_extraction = False\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if start_extraction:\n",
    "            if not line:  \n",
    "                break\n",
    "            parts = line.split(\"\\t\")\n",
    "            unigram = parts[1]  \n",
    "            unigrams.append(unigram)\n",
    "        elif line.startswith(\"\\\\1-grams:\"):\n",
    "            start_extraction = True\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for unigram in unigrams:\n",
    "        file.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1171c5a5-e6ac-41b6-bb42-33d935b5c35f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libri.apra\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43afe153-5c1f-4bb3-ab84-40e177b5bf4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
