{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "target_lang=\"en\"  # change to your target lang\n",
    "\n",
    "model_user = \"Aanchan\"\n",
    "model_repo = \"psst_model_cer_2\"\n",
    "model_name = f\"{model_user}/{model_repo}\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "username = \"jindaznb\"  # change to your username\n",
    "\n",
    "ngram_order = 3\n",
    "repo_user = \"jindaznb\"\n",
    "repo_name = f\"europarl_bilingual_kenlm_{ngram_order}-gram_phoneme\"\n",
    "repo_path_remote = f\"{repo_user}/{repo_name}\"\n",
    "repo_path_local = f\"{repo_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in the file: 990161\n"
     ]
    }
   ],
   "source": [
    "file_path = \"dataset_phonemes.txt\"\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "num_lines = len(lines)\n",
    "print(\"Number of lines in the file:\", num_lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2de838ed384fd88bfa06d540ef2307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "!git config --global user.email \"jindaz.work@outlook.com\"\n",
    "!git config --global user.name \"j\"\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/europarl_bilingual_kenlm_3-gram_phoneme is already a clone of https://huggingface.co/jindaznb/europarl_bilingual_kenlm_3-gram_phoneme. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Repository jindaznb/europarl_bilingual_kenlm_3-gram_phoneme already exists.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import create_repo, Repository\n",
    "\n",
    "try:\n",
    "  repo = Repository(repo_path_local, clone_from=repo_path_remote)\n",
    "  print(f\"\\nRepository {repo_path_remote} already exists.\")\n",
    "except:\n",
    "  create_repo(repo_path_remote, exist_ok=True)\n",
    "  repo = Repository(repo_path_local, clone_from=repo_path_remote)\n",
    "  print(\"\\nRepository created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train wordlevel ngram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/dataset_phonemes.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 104269206 types 48\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:576 2:15701515264 3:29440344064 4:47104544768 5:68694134784\n",
      "Substituting fallback discounts for order 0: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 48 D1=0.5 D2=1 D3+=1.5\n",
      "2 1438 D1=0.503546 D2=0.532523 D3+=0.0379641\n",
      "3 27827 D1=0.454491 D2=0.987406 D3+=1.49656\n",
      "4 274963 D1=0.519184 D2=1.04709 D3+=1.56705\n",
      "5 1397737 D1=0.548146 D2=1.0195 D3+=1.4856\n",
      "Memory estimate for binary LM:\n",
      "type       kB\n",
      "probing 31701 assuming -p 1.5\n",
      "probing 33483 assuming -r models -p 1.5\n",
      "trie     9648 without quantization\n",
      "trie     3985 assuming -q 8 -b 8 quantization \n",
      "trie     9224 assuming -a 22 array pointer compression\n",
      "trie     3561 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:576 2:23008 3:556540 4:6599112 5:39136636\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:576 2:23008 3:556540 4:6599112 5:39136636\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:157323268 kB\tVmRSS:10900 kB\tRSSMax:27773644 kB\tuser:14.5492\tsys:14.4147\tCPU:28.964\treal:29.3797\n"
     ]
    }
   ],
   "source": [
    "# !kenlm/build/bin/lmplz -o 5 <\"dataset_phonemes.txt\" > \"5gram.arpa\" --discount_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open ‘5gram.arpa’ for reading: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!head -20 5gram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"5gram.arpa\", \"r\") as read_file, open(\"5gram_correct.arpa\", \"w\") as write_file:\n",
    "#   has_added_eos = False\n",
    "#   for line in read_file:\n",
    "#     if not has_added_eos and \"ngram 1=\" in line:\n",
    "#       count=line.strip().split(\"=\")[-1]\n",
    "#       write_file.write(line.replace(f\"{count}\", f\"{int(count)+1}\"))\n",
    "#     elif not has_added_eos and \"<s>\" in line:\n",
    "#       write_file.write(line)\n",
    "#       write_file.write(line.replace(\"<s>\", \"</s>\"))\n",
    "#       has_added_eos = True\n",
    "#     else:\n",
    "#       write_file.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading europarl_bilingual_kenlm_3-gram_phoneme/5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# if ngram_order > 1:\n",
    "#   !kenlm/build/bin/build_binary {repo_path_local}/5gram.arpa {repo_path_local}/5gram.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding files tracked by Git LFS: ['5gram.arpa']. This may take a bit of time if the files are large.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097f71d1b0624524ad186f55d1362033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file 5gram.arpa:   0%|          | 1.00/40.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53737a8036354edc8c74f1eecf16d957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file 5gram.bin:   0%|          | 1.00/31.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/jindaznb/europarl_bilingual_kenlm_3-gram_phoneme\n",
      "   9ce5853..3ae4dcb  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/jindaznb/europarl_bilingual_kenlm_3-gram_phoneme/commit/3ae4dcbfe060b21b4591c2498e47ac739040e5b1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo.push_to_hub(commit_message=\"Upload n-gram model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<pad>': 0,\n",
       " '<sil>': 41,\n",
       " '<spn>': 42,\n",
       " '<unk>': 43,\n",
       " 'AA': 1,\n",
       " 'AE': 2,\n",
       " 'AH': 3,\n",
       " 'AO': 4,\n",
       " 'AW': 5,\n",
       " 'AY': 6,\n",
       " 'B': 7,\n",
       " 'CH': 8,\n",
       " 'D': 9,\n",
       " 'DH': 10,\n",
       " 'DX': 11,\n",
       " 'EH': 12,\n",
       " 'ER': 13,\n",
       " 'EY': 14,\n",
       " 'F': 15,\n",
       " 'G': 16,\n",
       " 'HH': 17,\n",
       " 'IH': 18,\n",
       " 'IY': 19,\n",
       " 'JH': 20,\n",
       " 'K': 21,\n",
       " 'L': 22,\n",
       " 'M': 23,\n",
       " 'N': 24,\n",
       " 'NG': 25,\n",
       " 'OW': 26,\n",
       " 'OY': 27,\n",
       " 'P': 28,\n",
       " 'R': 29,\n",
       " 'S': 30,\n",
       " 'SH': 31,\n",
       " 'T': 32,\n",
       " 'TH': 33,\n",
       " 'UH': 34,\n",
       " 'UW': 35,\n",
       " 'V': 36,\n",
       " 'W': 37,\n",
       " 'Y': 38,\n",
       " 'Z': 39,\n",
       " 'ZH': 40,\n",
       " '<s>': 44,\n",
       " '</s>': 45}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_dict = processor.tokenizer.get_vocab()\n",
    "sorted_vocab_dict = {k.lower(): v for k, v in sorted(vocab_dict.items(), key=lambda item: item[1])}\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the LM will be faster if you build a binary file.\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/europarl_bilingual_kenlm_3-gram_phoneme/5gram.arpa\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Found entries of length > 1 in alphabet. This is unusual unless style is BPE, but the alphabet was not recognized as BPE type. Is this correct?\n",
      "Space token ' ' missing from vocabulary.\n",
      "Unigrams and labels don't seem to agree.\n",
      "Only 48 unigrams passed as vocabulary. Is this small or artificial data?\n"
     ]
    }
   ],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n",
    "\n",
    "decoder = build_ctcdecoder(\n",
    "    labels=list(sorted_vocab_dict.keys()),\n",
    "    kenlm_model_path=\"europarl_bilingual_kenlm_3-gram_phoneme/5gram.arpa\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The tokens {'F', 'V', 'EH', 'L', 'IY', 'AY', 'EY', 'DH', 'TH', 'CH', 'R', 'Z', 'AH', 'Y', 'P', 'SH', 'OY', 'OW', 'W', 'UW', 'B', 'G', 'UH', 'N', 'ZH', 'NG', 'HH', 'D', 'DX', 'AA', 'M', 'AE', 'AW', 'AO', 'IH', 'T', 'JH', 'K', 'ER', 'S'} are defined in the tokenizer's vocabulary, but not in the decoder's alphabet. Make sure to include {'F', 'V', 'EH', 'L', 'IY', 'AY', 'EY', 'DH', 'TH', 'CH', 'R', 'Z', 'AH', 'Y', 'P', 'SH', 'OY', 'OW', 'W', 'UW', 'B', 'G', 'UH', 'N', 'ZH', 'NG', 'HH', 'D', 'DX', 'AA', 'M', 'AE', 'AW', 'AO', 'IH', 'T', 'JH', 'K', 'ER', 'S'} in the decoder's alphabet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ProcessorWithLM\n\u001b[0;32m----> 3\u001b[0m processor_with_lm \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2ProcessorWithLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:95\u001b[0m, in \u001b[0;36mWav2Vec2ProcessorWithLM.__init__\u001b[0;34m(self, feature_extractor, tokenizer, decoder)\u001b[0m\n\u001b[1;32m     93\u001b[0m missing_decoder_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_missing_alphabet_tokens(decoder, tokenizer)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_decoder_tokens) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe tokens \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_decoder_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are defined in the tokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvocabulary, but not in the decoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms alphabet. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure to include \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_decoder_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the decoder\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms alphabet.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder \u001b[38;5;241m=\u001b[39m decoder\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_extractor\n",
      "\u001b[0;31mValueError\u001b[0m: The tokens {'F', 'V', 'EH', 'L', 'IY', 'AY', 'EY', 'DH', 'TH', 'CH', 'R', 'Z', 'AH', 'Y', 'P', 'SH', 'OY', 'OW', 'W', 'UW', 'B', 'G', 'UH', 'N', 'ZH', 'NG', 'HH', 'D', 'DX', 'AA', 'M', 'AE', 'AW', 'AO', 'IH', 'T', 'JH', 'K', 'ER', 'S'} are defined in the tokenizer's vocabulary, but not in the decoder's alphabet. Make sure to include {'F', 'V', 'EH', 'L', 'IY', 'AY', 'EY', 'DH', 'TH', 'CH', 'R', 'Z', 'AH', 'Y', 'P', 'SH', 'OY', 'OW', 'W', 'UW', 'B', 'G', 'UH', 'N', 'ZH', 'NG', 'HH', 'D', 'DX', 'AA', 'M', 'AE', 'AW', 'AO', 'IH', 'T', 'JH', 'K', 'ER', 'S'} in the decoder's alphabet."
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor_with_lm = Wav2Vec2ProcessorWithLM(\n",
    "    feature_extractor=processor.feature_extractor,\n",
    "    tokenizer=processor.tokenizer,\n",
    "    decoder=decoder\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'Repository' (from 'huggingface_hub.repository') is deprecated and will be removed from version '1.0'. Please prefer the http-based alternatives instead. Given its large adoption in legacy code, the complete removal is only planned on next major release.\n",
      "For more details, please read https://huggingface.co/docs/huggingface_hub/concepts/git_vs_http.\n",
      "  warnings.warn(warning_message, FutureWarning)\n",
      "Cloning https://huggingface.co/Aanchan/psst_model_cer_2 into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25cc0975e26241fb9ed9a360459dcb1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file pytorch_model.bin:   0%|          | 1.40k/360M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1dd2fc557354d9da43f69a4eb5e54c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Download file training_args.bin:  97%|#########7| 3.34k/3.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9959cc0d0e046bbb85d59ca2f775f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file training_args.bin:  29%|##9       | 1.00k/3.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "115692704b354ff8a468f681792d3372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Clean file pytorch_model.bin:   0%|          | 1.00k/360M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "repo = Repository(local_dir=\"model_staging_phoneme\", clone_from=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'processor_with_lm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprocessor_with_lm\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_staging_phoneme\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'processor_with_lm' is not defined"
     ]
    }
   ],
   "source": [
    "processor_with_lm.save_pretrained(\"model_staging_phoneme\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('torgo.csv')\n",
    "df['audio'] = df['audio'].apply(lambda x: f\"/work/van-speech-nlp/data/torgo/{x}\")\n",
    "df.to_csv('torgo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
