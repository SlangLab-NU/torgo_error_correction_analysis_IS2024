{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83a11ac7-d254-4d40-b22f-fe3069c6deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51c513c7-12f9-4e83-a0e3-b619f10606db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo.csv')\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eca91a71-f178-4c0e-b857-98239cf6bf75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['session', 'audio', 'text', 'speaker_id'],\n",
       "        num_rows: 16394\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f10b482f-53e3-4b34-bc12-b082f36766e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 228\n",
      "F03: 1075\n",
      "F04: 667\n",
      "M01: 739\n",
      "M02: 766\n",
      "M03: 800\n",
      "M04: 652\n",
      "M05: 573\n"
     ]
    }
   ],
   "source": [
    "atypical_speaker_texts = {}\n",
    "\n",
    "for speaker_id in atypical_speakers:\n",
    "    atypical_speaker_data = dataset_csv['train'].filter(lambda x: x['speaker_id'] == speaker_id)\n",
    "    texts = atypical_speaker_data['text']\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "    atypical_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    print(f\"{speaker_id}: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f0e6735f-0937-4eba-9d41-d433f0ec1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"atypical_speaker_texts\", exist_ok=True)\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    with open(f\"atypical_speaker_texts/{speaker_id}_texts.txt\", \"w\") as file:\n",
    "        for text in texts:\n",
    "            file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "66e1d06d-a8b7-4e79-a754-dee17cad7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/F01_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 556 types 193\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:2316 2:4670902784 3:8757943296\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 193 D1=0.857868 D2=1.63234 D3+=1.28426\n",
      "2 321 D1=0.5 D2=1 D3+=1.5\n",
      "3 231 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type        B\n",
      "probing 16876 assuming -p 1.5\n",
      "probing 19576 assuming -r models -p 1.5\n",
      "trie     8999 without quantization\n",
      "trie     9520 assuming -q 8 -b 8 quantization \n",
      "trie     8957 assuming -a 22 array pointer compression\n",
      "trie     9479 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:2316 2:5136 3:4620\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:2316 2:5136 3:4620\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13263752 kB\tVmRSS:3636 kB\tRSSMax:3024244 kB\tuser:0.457555\tsys:1.68505\tCPU:2.14266\treal:2.15633\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/F03_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 2772 types 771\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:9252 2:4670900224 3:8757938176\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 771 D1=0.813131 D2=1.27477 D3+=1.66942\n",
      "2 1483 D1=0.5 D2=1 D3+=1.5\n",
      "3 1163 D1=0.00253934 D2=1.99998 D3+=2.32453\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 74 assuming -p 1.5\n",
      "probing 86 assuming -r models -p 1.5\n",
      "trie    39 without quantization\n",
      "trie    30 assuming -q 8 -b 8 quantization \n",
      "trie    38 assuming -a 22 array pointer compression\n",
      "trie    29 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:9252 2:23728 3:23260\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:9252 2:23728 3:23260\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13267868 kB\tVmRSS:3744 kB\tRSSMax:3026456 kB\tuser:0.42978\tsys:1.68814\tCPU:2.11798\treal:2.13484\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/F04_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1700 types 672\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:8064 2:4670900736 3:8757939200\n",
      "Statistics:\n",
      "1 672 D1=0.831884 D2=1.31154 D3+=1.75217\n",
      "2 1242 D1=0.754982 D2=1.74076 D3+=0.615848\n",
      "3 958 D1=0.217975 D2=1.98318 D3+=1.60496\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 63 assuming -p 1.5\n",
      "probing 72 assuming -r models -p 1.5\n",
      "trie    33 without quantization\n",
      "trie    26 assuming -q 8 -b 8 quantization \n",
      "trie    32 assuming -a 22 array pointer compression\n",
      "trie    25 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:8064 2:19872 3:19160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:8064 2:19872 3:19160\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13265812 kB\tVmRSS:3728 kB\tRSSMax:3026076 kB\tuser:0.447728\tsys:1.58357\tCPU:2.03137\treal:2.05352\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/M01_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1849 types 608\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:7296 2:4670901248 3:8757940224\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 608 D1=0.8336 D2=1.27862 D3+=2.11083\n",
      "2 1113 D1=0.5 D2=1 D3+=1.5\n",
      "3 850 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 56 assuming -p 1.5\n",
      "probing 65 assuming -r models -p 1.5\n",
      "trie    29 without quantization\n",
      "trie    24 assuming -q 8 -b 8 quantization \n",
      "trie    29 assuming -a 22 array pointer compression\n",
      "trie    23 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:7296 2:17808 3:17000\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:7296 2:17808 3:17000\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13267864 kB\tVmRSS:3712 kB\tRSSMax:3025912 kB\tuser:0.421428\tsys:1.67673\tCPU:2.09822\treal:2.11045\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/M02_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1892 types 625\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:7500 2:4670901248 3:8757940224\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 625 D1=0.835148 D2=1.33818 D3+=1.80693\n",
      "2 1146 D1=0.5 D2=1 D3+=1.5\n",
      "3 876 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 58 assuming -p 1.5\n",
      "probing 67 assuming -r models -p 1.5\n",
      "trie    30 without quantization\n",
      "trie    24 assuming -q 8 -b 8 quantization \n",
      "trie    30 assuming -a 22 array pointer compression\n",
      "trie    24 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:7500 2:18336 3:17520\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:7500 2:18336 3:17520\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13267864 kB\tVmRSS:3716 kB\tRSSMax:3025960 kB\tuser:0.398896\tsys:1.64957\tCPU:2.0485\treal:2.0619\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/M03_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 2016 types 648\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:7776 2:4670901248 3:8757940224\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 648 D1=0.832084 D2=1.37594 D3+=2.04905\n",
      "2 1193 D1=0.5 D2=1 D3+=1.5\n",
      "3 905 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 60 assuming -p 1.5\n",
      "probing 69 assuming -r models -p 1.5\n",
      "trie    31 without quantization\n",
      "trie    25 assuming -q 8 -b 8 quantization \n",
      "trie    31 assuming -a 22 array pointer compression\n",
      "trie    25 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:7776 2:19088 3:18100\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:7776 2:19088 3:18100\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13267864 kB\tVmRSS:3720 kB\tRSSMax:3026048 kB\tuser:0.426554\tsys:1.67625\tCPU:2.10286\treal:2.10966\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/M04_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1654 types 453\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:5436 2:4670901760 3:8757941248\n",
      "Statistics:\n",
      "1 453 D1=0.812766 D2=1.55667 D3+=0.155319\n",
      "2 820 D1=0.755915 D2=0.449602 D3+=2.2328\n",
      "3 631 D1=0.00849849 D2=1.9847 D3+=2.99498\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 41 assuming -p 1.5\n",
      "probing 48 assuming -r models -p 1.5\n",
      "trie    21 without quantization\n",
      "trie    18 assuming -q 8 -b 8 quantization \n",
      "trie    21 assuming -a 22 array pointer compression\n",
      "trie    18 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:5436 2:13120 3:12620\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:5436 2:13120 3:12620\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13265812 kB\tVmRSS:3676 kB\tRSSMax:3025344 kB\tuser:0.447785\tsys:1.64821\tCPU:2.09606\treal:2.11322\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/atypical_speaker_texts/M05_texts.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 1417 types 721\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:8652 2:4670900736 3:8757939200\n",
      "Statistics:\n",
      "1 721 D1=0.825269 D2=1.46675 D3+=1.34946\n",
      "2 1350 D1=0.863799 D2=1.37261 D3+=1.04706\n",
      "3 1044 D1=0.58209 D2=1.8884 D3+=1.49342\n",
      "Memory estimate for binary LM:\n",
      "type    kB\n",
      "probing 68 assuming -p 1.5\n",
      "probing 79 assuming -r models -p 1.5\n",
      "trie    36 without quantization\n",
      "trie    28 assuming -q 8 -b 8 quantization \n",
      "trie    35 assuming -a 22 array pointer compression\n",
      "trie    27 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:8652 2:21600 3:20880\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:8652 2:21600 3:20880\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:13267860 kB\tVmRSS:3736 kB\tRSSMax:3026372 kB\tuser:0.392586\tsys:1.7042\tCPU:2.09682\treal:2.10778\n"
     ]
    }
   ],
   "source": [
    "arpa_dir = \"arpa_files\"\n",
    "os.makedirs(arpa_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(\"atypical_speaker_texts\"):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        speaker_id = file_name.split(\"_\")[0]\n",
    "        arpa_file = f\"{arpa_dir}/{speaker_id}_3gram.arpa\"\n",
    "        txt_file = f\"atypical_speaker_texts/{file_name}\"\n",
    "        !kenlm/build/bin/lmplz -o 3 < \"{txt_file}\" > \"{arpa_file}\" -S 10% --discount_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3189db50-781e-46e3-be87-01294ff1d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']\n",
    "\n",
    "for speaker in atypical_speakers:\n",
    "    arpa_file = f\"torgo_arpa_files/{speaker}_3gram.arpa\"\n",
    "    output_file = f\"torgo_arpa_files/{speaker}_unigram.txt\"\n",
    "\n",
    "    unigrams = []\n",
    "\n",
    "    with open(arpa_file, \"r\") as file:\n",
    "        start_extraction = False\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if start_extraction:\n",
    "                if not line:  \n",
    "                    break\n",
    "                parts = line.split(\"\\t\")\n",
    "                unigram = parts[1]  \n",
    "                unigrams.append(unigram)\n",
    "            elif line.startswith(\"\\\\1-grams:\"):\n",
    "                start_extraction = True\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for unigram in unigrams:\n",
    "            file.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f428d-6e04-48dc-9892-225df8f573a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
