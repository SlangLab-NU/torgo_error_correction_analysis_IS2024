{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8e4d83-4570-422d-851b-cb99e59ab273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2dbde2-508c-4d39-98ab-3cdb67d060b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speakers = ['MC01', 'MC02', 'MC03', 'MC04','FC01','FC02','FC03']\n",
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44ab2c-d401-4e43-8718-6a46506d6758",
   "metadata": {},
   "source": [
    "# read europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ca931d-80f3-4211-9e37-7cea3f2380a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl = load_dataset(f\"{username}/{source_lang}_corpora_parliament_processed\", split=\"train\",cache_dir=\"/work/van-speech-nlp/cache\")\n",
    "translations=europarl['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ab4eb5b-efff-4794-9cbd-e032ca638eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051014"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86e09b8a-5494-4aed-b882-c7ba857c735a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# total_tokens = 0\n",
    "\n",
    "# # Iterate over each sentence in the translations corpus\n",
    "# for sentence in translations:\n",
    "#     # Tokenize the sentence into individual words\n",
    "#     tokens = nltk.word_tokenize(sentence)\n",
    "#     # Add the number of tokens in the current sentence to the total count\n",
    "#     total_tokens += len(tokens)\n",
    "\n",
    "# # Print the total number of tokens in the translations corpus\n",
    "# print(\"Total number of tokens in corpus:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "341bbe8c-a2c7-4807-b377-c1e3fc0c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set_europarl = set()\n",
    "\n",
    "for sentence in translations:\n",
    "    words = sentence.split()\n",
    "    word_set_europarl.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05212f13-66ee-41da-8c4a-56be8561552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set_europarl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf7539a3-e913-4b8d-b604-509788b8709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_set_cleaned = set()\n",
    "\n",
    "for word in translations:\n",
    "    if word:\n",
    "        cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
    "        filtered_word_set_cleaned.add(cleaned_word)\n",
    "\n",
    "cleaned_word_count = len(filtered_word_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f652d0a-9811-4a50-883e-5228b47a1572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1994288"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df452-e4a2-4910-95e1-eb8f1768952a",
   "metadata": {},
   "source": [
    "# read torgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4cc33e5-9863-4f3a-96c8-f190e64de880",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"keep_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26792180-b682-4a11-ab3a-93ea53cead51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo_new.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo_new.csv')\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bee5d29-8727-40a1-99fc-f2ac4b58bfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Define a function to clean the text\n",
    "def clean_text(text):\n",
    "    # Remove non-alphanumeric characters and convert to lowercase\n",
    "    cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to the 'text' column in data_df\n",
    "data_df['text'] = data_df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e77a7ea-0d89-4d01-99c7-4f56106f93cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 126\n",
      "stick\n",
      "except in the winter when the ooze or snow or ice prevents\n",
      "up\n",
      "know\n",
      "he slowly takes a short walk in the open air each day\n",
      "double\n",
      "you wished to know all about my grandfather\n",
      "knee\n",
      "she had your dark suit in greasy wash water all year\n",
      "car\n",
      "\n",
      "F03: 592\n",
      "beta\n",
      "stubble\n",
      "stubble\n",
      "trace\n",
      "goat\n",
      "she had your dark suit in greasy wash water all year\n",
      "well he is nearly ninetythree years old\n",
      "knew\n",
      "sip\n",
      "alpha\n",
      "\n",
      "F04: 367\n",
      "fee\n",
      "yet he still thinks as swiftly as ever\n",
      "knew\n",
      "trait\n",
      "stubble\n",
      "you wished to know all about my grandfather\n",
      "chair\n",
      "trace\n",
      "go\n",
      "hair\n",
      "\n",
      "M01: 407\n",
      "when he speaks his voice is just a bit cracked and quivers a trifle\n",
      "trait\n",
      "trouble\n",
      "fee\n",
      "raid\n",
      "except in the winter when the ooze or snow or ice prevents\n",
      "a long flowing beard clings to his chin\n",
      "a long flowing beard clings to his chin\n",
      "fair\n",
      "bit\n",
      "\n",
      "M02: 423\n",
      "alpha\n",
      "sip\n",
      "dagger\n",
      "fair\n",
      "park\n",
      "bubble\n",
      "bit\n",
      "well he is nearly ninetythree years old\n",
      "trouble\n",
      "chair\n",
      "\n",
      "M03: 442\n",
      "now\n",
      "know\n",
      "gadget\n",
      "race\n",
      "well he is nearly ninetythree years old\n",
      "well he is nearly ninetythree years old\n",
      "hair\n",
      "alpha\n",
      "warm\n",
      "goat\n",
      "\n",
      "M04: 360\n",
      "trouble\n",
      "spark\n",
      "weed\n",
      "twice each day he plays skillfully and with zest upon our small organ\n",
      "knew\n",
      "knee\n",
      "sip\n",
      "jacket\n",
      "trade\n",
      "stick\n",
      "\n",
      "M05: 316\n",
      "swore\n",
      "up\n",
      "up\n",
      "twice each day he plays skillfully and with zest upon our small organ\n",
      "giving those who observe him a pronounced feeling of the utmost respect\n",
      "spark\n",
      "race\n",
      "well he is nearly ninetythree years old\n",
      "bit\n",
      "chair\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Initialize a defaultdict to store the texts for each speaker\n",
    "speaker_texts = defaultdict(list)\n",
    "\n",
    "# Iterate through each test speaker\n",
    "for test_speaker in atypical_speakers:\n",
    "    valid_speaker = 'F03' if test_speaker != 'F03' else 'F04'\n",
    "    train_speaker = [s for s in speakers if s not in [test_speaker, valid_speaker]]\n",
    "    torgo_dataset = DatasetDict()\n",
    "    torgo_dataset['train'] = dataset_csv['train'].filter(lambda x: x in train_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['validation'] = dataset_csv['train'].filter(lambda x: x == valid_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['test'] = dataset_csv['train'].filter(lambda x: x == test_speaker, input_columns=['speaker_id'])\n",
    "\n",
    "    torgo_dataset['train'] = torgo_dataset['train'].filter(lambda x: x['test_data'] == 0)\n",
    "    torgo_dataset['validation'] = torgo_dataset['validation'].filter(lambda x: x['test_data'] == 0)\n",
    "    torgo_dataset['test'] = torgo_dataset['test'].filter(lambda x: x['test_data'] == 1)\n",
    "\n",
    "    # Filter the DataFrame to get the data for the train split for the selected train speakers\n",
    "    test_data = torgo_dataset['test']\n",
    "    \n",
    "    # Append the texts from the filtered DataFrame to the corresponding speaker in the speaker_texts_train dictionary\n",
    "    for text in test_data['text']:\n",
    "        # print(text)\n",
    "        new_text=clean_text(text)\n",
    "        # print(new_text)\n",
    "        speaker_texts[test_speaker].append(new_text)\n",
    "\n",
    "# # Print the number of texts collected for each speaker\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    print(f\"{speaker_id}: {len(texts)}\")\n",
    "    for text in texts[:10]:\n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb2347-a2e7-4fff-8066-a9656bbcab0a",
   "metadata": {},
   "source": [
    "# calculate unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a936ba-0bca-412f-b83e-20b2a10af5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 117 unique words\n",
      "F03: 447 unique words\n",
      "F04: 420 unique words\n",
      "M01: 381 unique words\n",
      "M02: 392 unique words\n",
      "M03: 399 unique words\n",
      "M04: 283 unique words\n",
      "M05: 436 unique words\n"
     ]
    }
   ],
   "source": [
    "unique_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    unique_words = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
    "            if cleaned_word:  \n",
    "                unique_words.add(cleaned_word) \n",
    "    unique_words_per_speaker[speaker_id] = unique_words\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    print(f\"{speaker_id}: {len(unique_words)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbe8f9-43db-4b41-bfda-e760ff39795f",
   "metadata": {},
   "source": [
    "# vocabulary similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b5d037-a6c3-4fc6-99ed-fe6e2c2630a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 2.56% dissimilarity with word_set_europarl\n",
      "F01: out_of_domain_words count: 3\n",
      "Out of domain words for F01: {'quivers', 'stubble', 'ooze'}\n",
      "\n",
      "F03: 4.92% dissimilarity with word_set_europarl\n",
      "F03: out_of_domain_words count: 22\n",
      "Out of domain words for F03: {'woolen', 'dart', 'slicker', 'nancys', 'toot', 'youd', 'stubble', 'mut', 'fleecy', 'glitter', 'schoolhouse', 'ooze', 'hem', 'sitter', 'cupping', 'shimmers', 'quivers', 'advisement', 'droop', 'yell', 'briar', 'youre'}\n",
      "\n",
      "F04: 5.00% dissimilarity with word_set_europarl\n",
      "F04: out_of_domain_words count: 21\n",
      "Out of domain words for F04: {'woolen', 'dart', 'slicker', 'nancys', 'toot', 'youd', 'stubble', 'mut', 'fleecy', 'frock', 'schoolhouse', 'ooze', 'hem', 'shimmers', 'quivers', 'advisement', 'droop', 'yell', 'briar', 'youre', 'lilyrare'}\n",
      "\n",
      "M01: 4.72% dissimilarity with word_set_europarl\n",
      "M01: out_of_domain_words count: 18\n",
      "Out of domain words for M01: {'woolen', 'stubble', 'dart', 'quivers', 'nancys', 'mut', 'advisement', 'fleecy', 'droop', 'yell', 'briar', 'toot', 'ooze', 'youre', 'youd', 'schoolhouse', 'hem', 'shimmers'}\n",
      "\n",
      "M02: 4.85% dissimilarity with word_set_europarl\n",
      "M02: out_of_domain_words count: 19\n",
      "Out of domain words for M02: {'woolen', 'dart', 'nancys', 'toot', 'youd', 'stubble', 'mut', 'fleecy', 'schoolhouse', 'ooze', 'hem', 'sitter', 'shimmers', 'quivers', 'advisement', 'droop', 'yell', 'briar', 'youre'}\n",
      "\n",
      "M03: 4.76% dissimilarity with word_set_europarl\n",
      "M03: out_of_domain_words count: 19\n",
      "Out of domain words for M03: {'woolen', 'dart', 'slicker', 'nancys', 'toot', 'youd', 'stubble', 'mut', 'fleecy', 'schoolhouse', 'ooze', 'hem', 'shimmers', 'quivers', 'advisement', 'droop', 'yell', 'briar', 'youre'}\n",
      "\n",
      "M04: 5.30% dissimilarity with word_set_europarl\n",
      "M04: out_of_domain_words count: 15\n",
      "Out of domain words for M04: {'woolen', 'quivers', 'stubble', 'mut', 'nancys', 'advisement', 'fleecy', 'droop', 'yell', 'briar', 'toot', 'ooze', 'youre', 'youd', 'shimmers'}\n",
      "\n",
      "M05: 4.82% dissimilarity with word_set_europarl\n",
      "M05: out_of_domain_words count: 21\n",
      "Out of domain words for M05: {'woolen', 'dart', 'slicker', 'nancys', 'toot', 'youd', 'stubble', 'mut', 'fleecy', 'glitter', 'schoolhouse', 'ooze', 'hem', 'sitter', 'shimmers', 'quivers', 'advisement', 'droop', 'yell', 'briar', 'youre'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_of_domain_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    overlap = len(unique_words & word_set_europarl)\n",
    "    similarity = overlap / len(unique_words) if len(unique_words) > 0 else 0\n",
    "    out_of_domain_similarity = (1 - similarity)*100\n",
    "\n",
    "    out_of_domain_words = {word for word in unique_words if word not in word_set_europarl}\n",
    "    out_of_domain_words_per_speaker[speaker_id] = out_of_domain_words\n",
    "\n",
    "    print(f\"{speaker_id}: {out_of_domain_similarity:.2f}% dissimilarity with word_set_europarl\")\n",
    "    print(f\"{speaker_id}: out_of_domain_words count: {len(out_of_domain_words)}\")\n",
    "    print(f\"Out of domain words for {speaker_id}: {out_of_domain_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e061463-697d-4da9-ba06-e060691eae8c",
   "metadata": {},
   "source": [
    "# Out of domain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5ca79b25-8f46-4b56-a927-acef8b00a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445.298548730424\n",
      "13.064232719071327\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "model=kenlm.Model(\"europarl_lm_arpa_files/3gram.bin\") \n",
    "per=model.perplexity(\"your text sentance\")\n",
    "print(per)\n",
    "\n",
    "per=model.perplexity(\"resumption of the session\")\n",
    "print(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae7fba71-6e51-412e-acd9-24d3e255e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01: Average Perplexity: 928.958100079841\n",
      "Speaker F03: Average Perplexity: 1609.073922518608\n",
      "Speaker F04: Average Perplexity: 1663.791194970111\n",
      "Speaker M01: Average Perplexity: 1618.6761751822528\n",
      "Speaker M02: Average Perplexity: 1562.3229905124986\n",
      "Speaker M03: Average Perplexity: 1526.6267614069698\n",
      "Speaker M04: Average Perplexity: 1498.6719645933758\n",
      "Speaker M05: Average Perplexity: 1431.2883772418654\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store perplexities per speaker\n",
    "perplexities_per_speaker = {}\n",
    "\n",
    "# Calculate perplexity for each speaker\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    perplexities = []\n",
    "    for text in texts:\n",
    "        perplexity = model.perplexity(text)\n",
    "        perplexities.append(perplexity)\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    perplexities_per_speaker[speaker_id] = avg_perplexity\n",
    "\n",
    "# Print average perplexity for each speaker\n",
    "for speaker_id, avg_perplexity in perplexities_per_speaker.items():\n",
    "    print(f\"Speaker {speaker_id}: Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e92ef-0c28-4592-9dd0-876487576e92",
   "metadata": {},
   "source": [
    "# Average Perplexity: 1498.1788760387824 showing that TORGO is out of domain text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15858ed1-b922-41f0-948d-a2455abaccea",
   "metadata": {},
   "source": [
    "# clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce1fc610-f314-499d-891a-b150c07f3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01:\n",
      "stick\n",
      "except in the winter when the ooze or snow or ice prevents\n",
      "\n",
      "Speaker F03:\n",
      "beta\n",
      "stubble\n",
      "\n",
      "Speaker F04:\n",
      "fee\n",
      "yet he still thinks as swiftly as ever\n",
      "\n",
      "Speaker M01:\n",
      "when he speaks his voice is just a bit cracked and quivers a trifle\n",
      "trait\n",
      "\n",
      "Speaker M02:\n",
      "grow\n",
      "feed\n",
      "\n",
      "Speaker M03:\n",
      "now\n",
      "know\n",
      "\n",
      "Speaker M04:\n",
      "trouble\n",
      "spark\n",
      "\n",
      "Speaker M05:\n",
      "swore\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned_speaker_texts = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    cleaned_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    print(f\"Speaker {speaker_id}:\")\n",
    "    for text in texts[:2]:  \n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60dadda6-a29a-46f6-8613-e473e0278871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01: Average Perplexity: 1014.5739804044058\n",
      "Speaker F03: Average Perplexity: 1468.7080678251675\n",
      "Speaker F04: Average Perplexity: 1474.5388428766746\n",
      "Speaker M01: Average Perplexity: 1534.7643705710645\n",
      "Speaker M02: Average Perplexity: 1492.4557345179953\n",
      "Speaker M03: Average Perplexity: 1492.9785709867167\n",
      "Speaker M04: Average Perplexity: 1531.2270123869641\n",
      "Speaker M05: Average Perplexity: 1584.9380105002526\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store perplexities per speaker\n",
    "perplexities_per_speaker = {}\n",
    "\n",
    "# Calculate perplexity for each speaker\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    perplexities = []\n",
    "    for text in texts:\n",
    "        perplexity = model.perplexity(text)\n",
    "        perplexities.append(perplexity)\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    perplexities_per_speaker[speaker_id] = avg_perplexity\n",
    "\n",
    "# Print average perplexity for each speaker\n",
    "for speaker_id, avg_perplexity in perplexities_per_speaker.items():\n",
    "    print(f\"Speaker {speaker_id}: Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d644966-e731-4845-975f-3268b6806b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
