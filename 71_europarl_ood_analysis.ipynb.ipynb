{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f8e4d83-4570-422d-851b-cb99e59ab273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec2dbde2-508c-4d39-98ab-3cdb67d060b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_speakers = ['MC01', 'MC02', 'MC03', 'MC04','FC01','FC02','FC03']\n",
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44ab2c-d401-4e43-8718-6a46506d6758",
   "metadata": {},
   "source": [
    "# read europarl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1ca931d-80f3-4211-9e37-7cea3f2380a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24766de4f5e8484ca966a716f5b39de3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/172M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8945c4a456594bd8baec2806b5642d0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/2051014 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "europarl = load_dataset(f\"{username}/{source_lang}_corpora_parliament_processed\", split=\"train\",cache_dir=\"/work/van-speech-nlp/cache\")\n",
    "translations=europarl['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ab4eb5b-efff-4794-9cbd-e032ca638eb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2051014"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86e09b8a-5494-4aed-b882-c7ba857c735a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in corpus: 51953865\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "total_tokens = 0\n",
    "\n",
    "# Iterate over each sentence in the translations corpus\n",
    "for sentence in translations:\n",
    "    # Tokenize the sentence into individual words\n",
    "    tokens = nltk.word_tokenize(sentence)\n",
    "    # Add the number of tokens in the current sentence to the total count\n",
    "    total_tokens += len(tokens)\n",
    "\n",
    "# Print the total number of tokens in the translations corpus\n",
    "print(\"Total number of tokens in corpus:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341bbe8c-a2c7-4807-b377-c1e3fc0c10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_set_europarl = set()\n",
    "\n",
    "for sentence in translations:\n",
    "    words = sentence.split()\n",
    "    word_set_europarl.update(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05212f13-66ee-41da-8c4a-56be8561552a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120001"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_set_europarl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf7539a3-e913-4b8d-b604-509788b8709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_set_cleaned = set()\n",
    "\n",
    "for word in translations:\n",
    "    if word:\n",
    "        cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
    "        filtered_word_set_cleaned.add(cleaned_word)\n",
    "\n",
    "cleaned_word_count = len(filtered_word_set_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f652d0a-9811-4a50-883e-5228b47a1572",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1994288"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71df452-e4a2-4910-95e1-eb8f1768952a",
   "metadata": {},
   "source": [
    "# read torgo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4cc33e5-9863-4f3a-96c8-f190e64de880",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = \"keep_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26792180-b682-4a11-ab3a-93ea53cead51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15aebc708c4a48ffbf8ffd745dee961b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo.csv')\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bee5d29-8727-40a1-99fc-f2ac4b58bfb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0144ed7290774655aa562e8f08d4ae19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c89ef811c3144851b5635da718d8e7dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f7a9a67234c422593b545fa9ca98121",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3735232de56d4982815acbd472e6691f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a6048b8f245449fb37786e457580abc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75eb19d4f704bedba16df1390b050d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1809f58094c74274b9ad95d65bad57e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705ec88028474fab826258eac60e8a3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e730d78e2a6c47cf9c2039de717ddd30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2c77e78113c463b8642d370afc50995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fe0aa1a1fb14fec959495c1f07be296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1808fabfd7bc4738a3a1178bd0dcd0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4670c299a86842d083cf543a094c38ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ba19d2835a4411b57240f4eb478412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 15091\n",
      "F03: 14652\n",
      "F04: 14652\n",
      "M01: 14580\n",
      "M02: 14553\n",
      "M03: 14519\n",
      "M04: 14667\n",
      "M05: 14746\n"
     ]
    }
   ],
   "source": [
    "dataset_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eb2347-a2e7-4fff-8066-a9656bbcab0a",
   "metadata": {},
   "source": [
    "# calculate unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0a936ba-0bca-412f-b83e-20b2a10af5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 1561 unique words\n",
      "F03: 1557 unique words\n",
      "F04: 1557 unique words\n",
      "M01: 1559 unique words\n",
      "M02: 1557 unique words\n",
      "M03: 1561 unique words\n",
      "M04: 1559 unique words\n",
      "M05: 1557 unique words\n"
     ]
    }
   ],
   "source": [
    "unique_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    unique_words = set()\n",
    "    for text in texts:\n",
    "        words = text.split()\n",
    "        for word in words:\n",
    "            cleaned_word = re.sub(r'[^a-zA-Z0-9]', '', word.lower())\n",
    "            if cleaned_word:  \n",
    "                unique_words.add(cleaned_word) \n",
    "    unique_words_per_speaker[speaker_id] = unique_words\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    print(f\"{speaker_id}: {len(unique_words)} unique words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bbe8f9-43db-4b41-bfda-e760ff39795f",
   "metadata": {},
   "source": [
    "# vocabulary similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78b5d037-a6c3-4fc6-99ed-fe6e2c2630a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 5.70% dissimilarity with word_set_europarl\n",
      "F01: out_of_domain_words count: 89\n",
      "Out of domain words for F01: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "F03: 5.65% dissimilarity with word_set_europarl\n",
      "F03: out_of_domain_words count: 88\n",
      "Out of domain words for F03: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "F04: 5.65% dissimilarity with word_set_europarl\n",
      "F04: out_of_domain_words count: 88\n",
      "Out of domain words for F04: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "M01: 5.71% dissimilarity with word_set_europarl\n",
      "M01: out_of_domain_words count: 89\n",
      "Out of domain words for M01: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "M02: 5.72% dissimilarity with word_set_europarl\n",
      "M02: out_of_domain_words count: 89\n",
      "Out of domain words for M02: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "M03: 5.70% dissimilarity with word_set_europarl\n",
      "M03: out_of_domain_words count: 89\n",
      "Out of domain words for M03: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "M04: 5.71% dissimilarity with word_set_europarl\n",
      "M04: out_of_domain_words count: 89\n",
      "Out of domain words for M04: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n",
      "M05: 5.72% dissimilarity with word_set_europarl\n",
      "M05: out_of_domain_words count: 89\n",
      "Out of domain words for M05: {'dill', 'scrubbed', 'marvelously', 'ambidextrous', 'sitter', 'wack', 'werent', 'weve', 'owl', 'bongos', 'quivers', 'briar', 'touchdown', 'taxaided', 'lacy', 'gingham', 'postdate', 'musk', 'screech', 'woolen', 'youd', 'trespassers', 'ligament', 'porch', 'toot', 'nancys', 'bender', 'innocense', 'thigh', 'jims', 'shawl', 'whoop', 'calico', 'advisement', 'mall', 'slicker', 'combed', 'mabel', 'giggling', 'kat', 'solicits', 'glitter', 'aluminum', 'frock', 'accordion', 'rakes', 'arent', 'ooze', 'darn', 'yell', 'treehouse', 'droop', 'sprain', 'schoolhouse', 'bloat', 'mold', 'urchins', 'ail', 'xray', 'hem', 'fleecy', 'boone', 'renter', 'chowder', 'lilyrare', 'stubble', 'someplace', 'whitecapped', 'youre', 'mom', 'beep', 'bracelet', 'colorful', 'dart', 'porcupines', 'theyll', 'ladle', 'prowler', 'gash', 'snort', 'appetizers', 'pete', 'fender', 'coupe', 'bun', 'ive', 'mut', 'shimmers', 'redhanded'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "out_of_domain_words_per_speaker = {}\n",
    "\n",
    "for speaker_id, unique_words in unique_words_per_speaker.items():\n",
    "    overlap = len(unique_words & word_set_europarl)\n",
    "    similarity = overlap / len(unique_words) if len(unique_words) > 0 else 0\n",
    "    out_of_domain_similarity = (1 - similarity)*100\n",
    "\n",
    "    out_of_domain_words = {word for word in unique_words if word not in word_set_europarl}\n",
    "    out_of_domain_words_per_speaker[speaker_id] = out_of_domain_words\n",
    "\n",
    "    print(f\"{speaker_id}: {out_of_domain_similarity:.2f}% dissimilarity with word_set_europarl\")\n",
    "    print(f\"{speaker_id}: out_of_domain_words count: {len(out_of_domain_words)}\")\n",
    "    print(f\"Out of domain words for {speaker_id}: {out_of_domain_words}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e061463-697d-4da9-ba06-e060691eae8c",
   "metadata": {},
   "source": [
    "# Out of domain analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5ca79b25-8f46-4b56-a927-acef8b00a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2445.298548730424\n",
      "13.064232719071327\n"
     ]
    }
   ],
   "source": [
    "import kenlm\n",
    "\n",
    "model=kenlm.Model(\"kenlm_model_3gram_words_europarl/3gram.bin\") \n",
    "per=model.perplexity(\"your text sentance\")\n",
    "print(per)\n",
    "\n",
    "per=model.perplexity(\"resumption of the session\")\n",
    "print(per)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ae7fba71-6e51-412e-acd9-24d3e255e420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Perplexity: 1498.1788760387824\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity for each text\n",
    "perplexities = []\n",
    "for text in cleaned_texts:\n",
    "    perplexity = model.perplexity(text)\n",
    "    perplexities.append(perplexity)\n",
    "\n",
    "# Calculate the average perplexity\n",
    "avg_perplexity = np.mean(perplexities)\n",
    "print(f\"Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0e92ef-0c28-4592-9dd0-876487576e92",
   "metadata": {},
   "source": [
    "# Average Perplexity: 1498.1788760387824 showing that TORGO is out of domain text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15858ed1-b922-41f0-948d-a2455abaccea",
   "metadata": {},
   "source": [
    "# clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ce1fc610-f314-499d-891a-b150c07f3c10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01:\n",
      "stick\n",
      "except in the winter when the ooze or snow or ice prevents\n",
      "\n",
      "Speaker F03:\n",
      "beta\n",
      "stubble\n",
      "\n",
      "Speaker F04:\n",
      "fee\n",
      "yet he still thinks as swiftly as ever\n",
      "\n",
      "Speaker M01:\n",
      "when he speaks his voice is just a bit cracked and quivers a trifle\n",
      "trait\n",
      "\n",
      "Speaker M02:\n",
      "grow\n",
      "feed\n",
      "\n",
      "Speaker M03:\n",
      "now\n",
      "know\n",
      "\n",
      "Speaker M04:\n",
      "trouble\n",
      "spark\n",
      "\n",
      "Speaker M05:\n",
      "swore\n",
      "train\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cleaned_speaker_texts = {}\n",
    "\n",
    "for speaker_id, texts in speaker_texts.items():\n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "    cleaned_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    print(f\"Speaker {speaker_id}:\")\n",
    "    for text in texts[:2]:  \n",
    "        print(text)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "60dadda6-a29a-46f6-8613-e473e0278871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speaker F01: Average Perplexity: 1014.5739804044058\n",
      "Speaker F03: Average Perplexity: 1468.7080678251675\n",
      "Speaker F04: Average Perplexity: 1474.5388428766746\n",
      "Speaker M01: Average Perplexity: 1534.7643705710645\n",
      "Speaker M02: Average Perplexity: 1492.4557345179953\n",
      "Speaker M03: Average Perplexity: 1492.9785709867167\n",
      "Speaker M04: Average Perplexity: 1531.2270123869641\n",
      "Speaker M05: Average Perplexity: 1584.9380105002526\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary to store perplexities per speaker\n",
    "perplexities_per_speaker = {}\n",
    "\n",
    "# Calculate perplexity for each speaker\n",
    "for speaker_id, texts in cleaned_speaker_texts.items():\n",
    "    perplexities = []\n",
    "    for text in texts:\n",
    "        perplexity = model.perplexity(text)\n",
    "        perplexities.append(perplexity)\n",
    "    avg_perplexity = np.mean(perplexities)\n",
    "    perplexities_per_speaker[speaker_id] = avg_perplexity\n",
    "\n",
    "# Print average perplexity for each speaker\n",
    "for speaker_id, avg_perplexity in perplexities_per_speaker.items():\n",
    "    print(f\"Speaker {speaker_id}: Average Perplexity: {avg_perplexity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d644966-e731-4845-975f-3268b6806b20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
