{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ec226c-c303-4e3c-accf-7cc978295ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyctcdecode import build_ctcdecoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a795199-dcbd-4064-a622-adcec817a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15fda4c-40a8-4cd6-a7f9-fbc53db54da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM\n",
    "# from pyctcdecode import build_ctcdecoder\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "import re\n",
    "from datasets import load_dataset\n",
    "#from g2p_en import G2p\n",
    "from g2p import make_g2p\n",
    "from num2words import num2words  # Import num2words from nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0601f8d0-d815-4327-8d07-b74c664661bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Repository\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 克隆仓库到本地目录 model_staging，并在您的命名空间下创建一个新的仓库\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m repo \u001b[38;5;241m=\u001b[39m Repository(local_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_staging\u001b[39m\u001b[38;5;124m\"\u001b[39m, clone_from\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_name\u001b[49m, organization\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_username\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import Repository\n",
    "\n",
    "# 克隆仓库到本地目录 model_staging，并在您的命名空间下创建一个新的仓库\n",
    "repo = Repository(local_dir=\"model_staging\", clone_from=model_name, organization=\"your_username\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8309165c-9ea1-4173-b2f2-ac4a90f93c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting soundfile\n",
      "  Downloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (1.26.3)\n",
      "Requirement already satisfied: scipy>=1.2.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (1.11.4)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (1.3.0)\n",
      "Requirement already satisfied: joblib>=0.14 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (1.2.0)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (0.59.0)\n",
      "Collecting pooch>=1.0 (from librosa)\n",
      "  Downloading pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (4.9.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: cffi>=1.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from soundfile) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from numba>=0.51.0->librosa) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from pooch>=1.0->librosa) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from pooch>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from pooch>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from scikit-learn>=0.20.0->librosa) (2.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2024.2.2)\n",
      "Downloading librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m988.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: soxr, audioread, soundfile, pooch, librosa\n",
      "Successfully installed audioread-3.0.1 librosa-0.10.1 pooch-1.8.1 soundfile-0.12.1 soxr-0.3.7\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58a79adc-f075-41fc-97ae-24b82398f23c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00_common.ipynb\n",
      "01_prepare_dataset.ipynb\n",
      "02_bart_no_keep.ipynb\n",
      "03_bart_keep-all.ipynb\n",
      "10_ngram_common.ipynb\n",
      "11_prepare_torgo_dataset.ipynb\n",
      "12_prepare_euproal_data.ipynb\n",
      "12_prepare_tatoeba_data-Copy1.ipynb\n",
      "13_jamspell_ec_europarl.ipynb\n",
      "13_jamspell_ec_tatoeba-Copy1.ipynb\n",
      "21_kenlm_lm.ipynb\n",
      "22_n-gram language model integration for fine-tuned xls-r53 models.ipynb\n",
      "\u001b[0m\u001b[01;34mdata\u001b[0m/\n",
      "dataset_europarl.txt\n",
      "dataset_phonemes.txt\n",
      "\u001b[01;34mdownloads\u001b[0m/\n",
      "\u001b[01;34meuroparl_bilingual_kenlm_3-gram\u001b[0m/\n",
      "europarl.txt\n",
      "\u001b[01;34mg2p-seq2seq\u001b[0m/\n",
      "\u001b[01;34mJamSpell\u001b[0m/\n",
      "\u001b[01;34mkenlm\u001b[0m/\n",
      "\u001b[01;34mlogs\u001b[0m/\n",
      "\u001b[01;34mmodel_staging\u001b[0m/\n",
      "output.csv\n",
      "\u001b[01;34mresults\u001b[0m/\n",
      "torgo.csv\n",
      "\u001b[01;34mtorgo_inference\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_F01\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_F03\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_F04\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_M01\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_M02\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_M03\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_M04\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_keep_all_M05\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_F01\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_F03\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_F04\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_M01\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_M02\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_M03\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_M04\u001b[0m/\n",
      "\u001b[01;34mtorgo_spell_correction_word_level_M05\u001b[0m/\n",
      "training_args.json\n",
      "train.py\n",
      "Untitled1.ipynb\n",
      "Untitled.ipynb\n",
      "\u001b[01;34mvocab\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "846d157f-fbff-4c21-a5d7-bca0eb0613f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_inference\n"
     ]
    }
   ],
   "source": [
    "cd torgo_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37ceae5b-865b-40db-b2b3-094403798293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting aiohttp==3.8.4 (from -r requirements.txt (line 1))\n",
      "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting aiosignal==1.3.1 (from -r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting async-timeout==4.0.2 (from -r requirements.txt (line 3))\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting attrs==22.2.0 (from -r requirements.txt (line 4))\n",
      "  Downloading attrs-22.2.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting audioread==3.0.0 (from -r requirements.txt (line 5))\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting certifi==2022.12.7 (from -r requirements.txt (line 6))\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting cffi==1.15.1 (from -r requirements.txt (line 7))\n",
      "  Downloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting charset-normalizer==3.0.1 (from -r requirements.txt (line 8))\n",
      "  Downloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
      "Collecting datasets==2.10.0 (from -r requirements.txt (line 9))\n",
      "  Downloading datasets-2.10.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator==5.1.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (5.1.1)\n",
      "Requirement already satisfied: dill==0.3.6 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.3.6)\n",
      "Collecting filelock==3.9.0 (from -r requirements.txt (line 12))\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting frozenlist==1.3.3 (from -r requirements.txt (line 13))\n",
      "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Collecting fsspec==2023.1.0 (from -r requirements.txt (line 14))\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting huggingface-hub==0.12.1 (from -r requirements.txt (line 15))\n",
      "  Downloading huggingface_hub-0.12.1-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting huggingsound==0.1.6 (from -r requirements.txt (line 16))\n",
      "  Downloading huggingsound-0.1.6-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: idna==3.4 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.4)\n",
      "Collecting jiwer==2.5.1 (from -r requirements.txt (line 18))\n",
      "  Downloading jiwer-2.5.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: joblib==1.2.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (1.2.0)\n",
      "Collecting Levenshtein==0.20.2 (from -r requirements.txt (line 20))\n",
      "  Downloading Levenshtein-0.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting librosa==0.9.2 (from -r requirements.txt (line 21))\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl.metadata (8.2 kB)\n",
      "Collecting llvmlite==0.39.1 (from -r requirements.txt (line 22))\n",
      "  Downloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: multidict==6.0.4 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (6.0.4)\n",
      "Requirement already satisfied: multiprocess==0.70.14 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 24)) (0.70.14)\n",
      "Collecting numba==0.56.4 (from -r requirements.txt (line 25))\n",
      "  Downloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.8 kB)\n",
      "Collecting numpy==1.23.5 (from -r requirements.txt (line 26))\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting packaging==23.0 (from -r requirements.txt (line 27))\n",
      "  Downloading packaging-23.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "Collecting pandas==1.5.3 (from -r requirements.txt (line 28))\n",
      "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting platformdirs==3.0.0 (from -r requirements.txt (line 29))\n",
      "  Downloading platformdirs-3.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting pooch==1.7.0 (from -r requirements.txt (line 30))\n",
      "  Downloading pooch-1.7.0-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting pyarrow==11.0.0 (from -r requirements.txt (line 31))\n",
      "  Downloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pycparser==2.21 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 32)) (2.21)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 33)) (2.8.2)\n",
      "Collecting pytz==2022.7.1 (from -r requirements.txt (line 34))\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl.metadata (21 kB)\n",
      "Collecting PyYAML==6.0 (from -r requirements.txt (line 35))\n",
      "  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting rapidfuzz==2.13.7 (from -r requirements.txt (line 36))\n",
      "  Downloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting regex==2022.10.31 (from -r requirements.txt (line 37))\n",
      "  Downloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m461.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting requests==2.28.2 (from -r requirements.txt (line 38))\n",
      "  Downloading requests-2.28.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting resampy==0.4.2 (from -r requirements.txt (line 39))\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting responses==0.18.0 (from -r requirements.txt (line 40))\n",
      "  Downloading responses-0.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting scikit-learn==1.2.1 (from -r requirements.txt (line 41))\n",
      "  Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy==1.10.1 (from -r requirements.txt (line 42))\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m658.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six==1.16.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 43)) (1.16.0)\n",
      "Requirement already satisfied: soundfile==0.12.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from -r requirements.txt (line 44)) (0.12.1)\n",
      "Collecting threadpoolctl==3.1.0 (from -r requirements.txt (line 45))\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting tokenizers==0.13.2 (from -r requirements.txt (line 46))\n",
      "  Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting torch==1.12.1 (from -r requirements.txt (line 47))\n",
      "  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n",
      "Collecting tqdm==4.64.1 (from -r requirements.txt (line 48))\n",
      "  Downloading tqdm-4.64.1-py2.py3-none-any.whl.metadata (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.3/57.3 kB\u001b[0m \u001b[31m519.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting transformers==4.23.1 (from -r requirements.txt (line 49))\n",
      "  Downloading transformers-4.23.1-py3-none-any.whl.metadata (88 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.7/88.7 kB\u001b[0m \u001b[31m691.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting typing_extensions==4.5.0 (from -r requirements.txt (line 50))\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting urllib3==1.26.14 (from -r requirements.txt (line 51))\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl.metadata (47 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m418.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting xxhash==3.2.0 (from -r requirements.txt (line 52))\n",
      "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting yarl==1.8.2 (from -r requirements.txt (line 53))\n",
      "  Downloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Requirement already satisfied: setuptools in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from numba==0.56.4->-r requirements.txt (line 25)) (68.2.2)\n",
      "Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading attrs-22.2.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m548.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (441 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.8/441.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading charset_normalizer-3.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (198 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m198.8/198.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.10.0-py3-none-any.whl (469 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.0/143.0 kB\u001b[0m \u001b[31m988.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingsound-0.1.6-py3-none-any.whl (28 kB)\n",
      "Downloading jiwer-2.5.1-py3-none-any.whl (15 kB)\n",
      "Downloading Levenshtein-0.20.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m214.3/214.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.39.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.6/34.6 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numba-0.56.4-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m432.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading platformdirs-3.0.0-py3-none-any.whl (14 kB)\n",
      "Downloading pooch-1.7.0-py3-none-any.whl (60 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m541.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.9/34.9 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.4/499.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-2.13.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2022.10.31-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (770 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m770.5/770.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m583.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Downloading scikit_learn-1.2.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading tokenizers-0.13.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl (776.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.3/776.3 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m629.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.23.1-py3-none-any.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading yarl-1.8.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.0/264.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23703 sha256=2e4e036a6a6e5f5796c51f725b42067e15fb46229a3f79cd9b099b1929c239e7\n",
      "  Stored in directory: /home/zhang.jinda1/.cache/pip/wheels/da/4b/39/c5f6c4ee93b43281dda4dab5ac5f2bdf9d11074d427493cd55\n",
      "Successfully built audioread\n",
      "Installing collected packages: tokenizers, pytz, charset-normalizer, yarl, xxhash, urllib3, typing_extensions, tqdm, threadpoolctl, regex, rapidfuzz, PyYAML, platformdirs, packaging, numpy, llvmlite, fsspec, frozenlist, filelock, cffi, certifi, audioread, attrs, async-timeout, torch, scipy, requests, pyarrow, pandas, numba, Levenshtein, aiosignal, scikit-learn, responses, resampy, pooch, jiwer, huggingface-hub, aiohttp, transformers, librosa, datasets, huggingsound\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~okenizers'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2023.3.post1\n",
      "    Uninstalling pytz-2023.3.post1:\n",
      "      Successfully uninstalled pytz-2023.3.post1\n",
      "  Attempting uninstall: charset-normalizer\n",
      "    Found existing installation: charset-normalizer 2.0.4\n",
      "    Uninstalling charset-normalizer-2.0.4:\n",
      "      Successfully uninstalled charset-normalizer-2.0.4\n",
      "  Attempting uninstall: yarl\n",
      "    Found existing installation: yarl 1.9.3\n",
      "    Uninstalling yarl-1.9.3:\n",
      "      Successfully uninstalled yarl-1.9.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~arl'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: xxhash\n",
      "    Found existing installation: xxhash 2.0.2\n",
      "    Uninstalling xxhash-2.0.2:\n",
      "      Successfully uninstalled xxhash-2.0.2\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~xhash'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.0.7\n",
      "    Uninstalling urllib3-2.0.7:\n",
      "      Successfully uninstalled urllib3-2.0.7\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.65.0\n",
      "    Uninstalling tqdm-4.65.0:\n",
      "      Successfully uninstalled tqdm-4.65.0\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 2.2.0\n",
      "    Uninstalling threadpoolctl-2.2.0:\n",
      "      Successfully uninstalled threadpoolctl-2.2.0\n",
      "  Attempting uninstall: regex\n",
      "    Found existing installation: regex 2023.10.3\n",
      "    Uninstalling regex-2023.10.3:\n",
      "      Successfully uninstalled regex-2023.10.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~egex'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: rapidfuzz\n",
      "    Found existing installation: rapidfuzz 3.6.1\n",
      "    Uninstalling rapidfuzz-3.6.1:\n",
      "      Successfully uninstalled rapidfuzz-3.6.1\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0.1\n",
      "    Uninstalling PyYAML-6.0.1:\n",
      "      Successfully uninstalled PyYAML-6.0.1\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~aml'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: platformdirs\n",
      "    Found existing installation: platformdirs 3.10.0\n",
      "    Uninstalling platformdirs-3.10.0:\n",
      "      Successfully uninstalled platformdirs-3.10.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.3\n",
      "    Uninstalling numpy-1.26.3:\n",
      "      Successfully uninstalled numpy-1.26.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~umpy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: llvmlite\n",
      "    Found existing installation: llvmlite 0.42.0\n",
      "    Uninstalling llvmlite-0.42.0:\n",
      "      Successfully uninstalled llvmlite-0.42.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "  Attempting uninstall: frozenlist\n",
      "    Found existing installation: frozenlist 1.4.0\n",
      "    Uninstalling frozenlist-1.4.0:\n",
      "      Successfully uninstalled frozenlist-1.4.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~rozenlist'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: filelock\n",
      "    Found existing installation: filelock 3.13.1\n",
      "    Uninstalling filelock-3.13.1:\n",
      "      Successfully uninstalled filelock-3.13.1\n",
      "  Attempting uninstall: cffi\n",
      "    Found existing installation: cffi 1.16.0\n",
      "    Uninstalling cffi-1.16.0:\n",
      "      Successfully uninstalled cffi-1.16.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2024.2.2\n",
      "    Uninstalling certifi-2024.2.2:\n",
      "      Successfully uninstalled certifi-2024.2.2\n",
      "  Attempting uninstall: audioread\n",
      "    Found existing installation: audioread 3.0.1\n",
      "    Uninstalling audioread-3.0.1:\n",
      "      Successfully uninstalled audioread-3.0.1\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.1.0\n",
      "    Uninstalling attrs-23.1.0:\n",
      "      Successfully uninstalled attrs-23.1.0\n",
      "  Attempting uninstall: async-timeout\n",
      "    Found existing installation: async-timeout 4.0.3\n",
      "    Uninstalling async-timeout-4.0.3:\n",
      "      Successfully uninstalled async-timeout-4.0.3\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.2.0\n",
      "    Uninstalling torch-2.2.0:\n",
      "      Successfully uninstalled torch-2.2.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~orch'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.11.4\n",
      "    Uninstalling scipy-1.11.4:\n",
      "      Successfully uninstalled scipy-1.11.4\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~cipy'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 14.0.2\n",
      "    Uninstalling pyarrow-14.0.2:\n",
      "      Successfully uninstalled pyarrow-14.0.2\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.1.4\n",
      "    Uninstalling pandas-2.1.4:\n",
      "      Successfully uninstalled pandas-2.1.4\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~andas'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.59.0\n",
      "    Uninstalling numba-0.59.0:\n",
      "      Successfully uninstalled numba-0.59.0\n",
      "  Attempting uninstall: aiosignal\n",
      "    Found existing installation: aiosignal 1.2.0\n",
      "    Uninstalling aiosignal-1.2.0:\n",
      "      Successfully uninstalled aiosignal-1.2.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.3.0\n",
      "    Uninstalling scikit-learn-1.3.0:\n",
      "      Successfully uninstalled scikit-learn-1.3.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~klearn'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: responses\n",
      "    Found existing installation: responses 0.13.3\n",
      "    Uninstalling responses-0.13.3:\n",
      "      Successfully uninstalled responses-0.13.3\n",
      "  Attempting uninstall: pooch\n",
      "    Found existing installation: pooch 1.8.1\n",
      "    Uninstalling pooch-1.8.1:\n",
      "      Successfully uninstalled pooch-1.8.1\n",
      "  Attempting uninstall: jiwer\n",
      "    Found existing installation: jiwer 3.0.3\n",
      "    Uninstalling jiwer-3.0.3:\n",
      "      Successfully uninstalled jiwer-3.0.3\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.20.3\n",
      "    Uninstalling huggingface-hub-0.20.3:\n",
      "      Successfully uninstalled huggingface-hub-0.20.3\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.3\n",
      "    Uninstalling aiohttp-3.9.3:\n",
      "      Successfully uninstalled aiohttp-3.9.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/~iohttp'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.32.1\n",
      "    Uninstalling transformers-4.32.1:\n",
      "      Successfully uninstalled transformers-4.32.1\n",
      "  Attempting uninstall: librosa\n",
      "    Found existing installation: librosa 0.10.1\n",
      "    Uninstalling librosa-0.10.1:\n",
      "      Successfully uninstalled librosa-0.10.1\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.17.0\n",
      "    Uninstalling datasets-2.17.0:\n",
      "      Successfully uninstalled datasets-2.17.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "sqlalchemy 2.0.25 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "jupyterlab-server 2.25.1 requires requests>=2.31, but you have requests 2.28.2 which is incompatible.\n",
      "pydantic 2.6.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.16.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "s3fs 2023.10.0 requires fsspec==2023.10.0, but you have fsspec 2023.1.0 which is incompatible.\n",
      "torchvision 0.17.0 requires torch==2.2.0, but you have torch 1.12.1 which is incompatible.\n",
      "google-api-core 2.17.1 requires google-auth<3.0.dev0,>=2.14.1, but you have google-auth 2.6.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Levenshtein-0.20.2 PyYAML-6.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 attrs-22.2.0 audioread-3.0.0 certifi-2022.12.7 cffi-1.15.1 charset-normalizer-3.0.1 datasets-2.10.0 filelock-3.9.0 frozenlist-1.3.3 fsspec-2023.1.0 huggingface-hub-0.12.1 huggingsound-0.1.6 jiwer-2.5.1 librosa-0.9.2 llvmlite-0.39.1 numba-0.56.4 numpy-1.23.5 packaging-23.0 pandas-1.5.3 platformdirs-3.0.0 pooch-1.7.0 pyarrow-11.0.0 pytz-2022.7.1 rapidfuzz-2.13.7 regex-2022.10.31 requests-2.28.2 resampy-0.4.2 responses-0.18.0 scikit-learn-1.2.1 scipy-1.10.1 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-1.12.1 tqdm-4.64.1 transformers-4.23.1 typing_extensions-4.5.0 urllib3-1.26.14 xxhash-3.2.0 yarl-1.8.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3aa6f9b-8eb8-478d-9537-2883c5d12ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_inference/Data Preparation\n"
     ]
    }
   ],
   "source": [
    "cd Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3793f3bb-d117-4129-a96b-216a965dde75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/23/2024 19:47:35 - INFO - huggingsound.speech_recognition.model - Loading model...\n",
      "Downloading (…)\"config.json\";: 100%|████████| 2.31k/2.31k [00:00<00:00, 500kB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_inference/Data Preparation/data_prep_spell_correction.py\", line 78, in <module>\n",
      "    main()\n",
      "  File \"/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_inference/Data Preparation/data_prep_spell_correction.py\", line 47, in main\n",
      "    model = SpeechRecognitionModel(source + speaker + \"-2\")\n",
      "  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingsound/speech_recognition/model.py\", line 55, in __init__\n",
      "    self._load_model()\n",
      "  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingsound/speech_recognition/model.py\", line 63, in _load_model\n",
      "    self.model = AutoModelForCTC.from_pretrained(self.model_path)\n",
      "  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py\", line 463, in from_pretrained\n",
      "    return model_class.from_pretrained(\n",
      "  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/modeling_utils.py\", line 2134, in from_pretrained\n",
      "    raise EnvironmentError(\n",
      "OSError: yip-i/torgo_xlsr_finetune-M04-2 does not appear to have a file named pytorch_model.bin, tf_model.h5, model.ckpt or flax_model.msgpack.\n"
     ]
    }
   ],
   "source": [
    "!python data_prep_spell_correction.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d025629-8da0-404a-9f32-25f7a7eb7822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (0.12.1)\n",
      "Requirement already satisfied: filelock in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: requests in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06da484f-2d71-4849-9461-d76360b8f793",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3533903109.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    huggingface-cli login --token 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\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4d7b07-3c13-4393-a08b-9cf5d263e09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: huggingface-cli <command> [<args>]\n",
      "huggingface-cli: error: unrecognized arguments: --token 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\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli login --token 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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "898d04d7-67d6-4b98-a442-fe174c477558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34menv\u001b[0m/  \u001b[01;34mF04\u001b[0m/   \u001b[01;34mFC03\u001b[0m/       \u001b[01;34mM01\u001b[0m/  \u001b[01;34mM04\u001b[0m/   \u001b[01;34mMC02\u001b[0m/  MC.tar.bz2  Torgo.zip\n",
      "\u001b[01;34mF01\u001b[0m/  \u001b[01;34mFC01\u001b[0m/  FC.tar.bz2  \u001b[01;34mM02\u001b[0m/  \u001b[01;34mM05\u001b[0m/   \u001b[01;34mMC03\u001b[0m/  M.tar.bz2\n",
      "\u001b[01;34mF03\u001b[0m/  \u001b[01;34mFC02\u001b[0m/  F.tar.bz2   \u001b[01;34mM03\u001b[0m/  \u001b[01;34mMC01\u001b[0m/  \u001b[01;34mMC04\u001b[0m/  Torgo.csv\n"
     ]
    }
   ],
   "source": [
    "ls /work/van-speech-nlp/data/torgo/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b0a3d15-a729-41b5-9dde-bdcae34f2b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/zhang.jinda1/.cache/huggingface/datasets/csv/default-8de5249d247f5548/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37a3446d72648a1bdaae1426d26b7ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590c9e84487c47e6bbd8389ab67037cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/zhang.jinda1/.cache/huggingface/datasets/csv/default-8de5249d247f5548/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/datasets/download/streaming_download_manager.py:776: FutureWarning: the 'mangle_dupe_cols' keyword is deprecated and will be removed in a future version. Please take steps to stop the use of 'mangle_dupe_cols'\n",
      "  return pd.read_csv(xopen(filepath_or_buffer, \"rb\", use_auth_token=use_auth_token), **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e658fd14e06e4b1d8ee21a8e64f88c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict, Dataset, Audio\n",
    "\n",
    "data = load_dataset('csv', data_files='torgo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9673d12-eeef-45b8-8f38-2c022cbade1d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e55565e2-40b9-40d1-945d-013a0d002d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sample = data['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bb2f90f-49c9-46c2-918d-512f2e738010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session': 'F01-Session1-arrayMic-0006',\n",
       " 'audio': '/F01/Session1/wav_arrayMic/0006.wav',\n",
       " 'text': 'STICK',\n",
       " 'speaker_id': 'F01'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce480ca7-e10a-430e-a9dd-bb5dff215812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset csv (/home/zhang.jinda1/.cache/huggingface/datasets/csv/default-8de5249d247f5548/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77594610be994e0ba315a79b356946f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16394 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'session': 'F01-Session1-arrayMic-0006', 'audio': '/work/van-speech-nlp/data/torgo/F01-Session1-arrayMic-0006', 'text': 'STICK', 'speaker_id': 'F01'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric, DatasetDict, Dataset, Audio\n",
    "\n",
    "# 加载数据集\n",
    "data = load_dataset('csv', data_files='torgo.csv')\n",
    "\n",
    "# 定义修改函数\n",
    "def modify_audio_path(example):\n",
    "    example['audio'] = f\"/work/van-speech-nlp/data/torgo/{example['session']}\"\n",
    "    return example\n",
    "\n",
    "# 使用 map 函数修改数据集中的 \"audio\" 列\n",
    "modified_data = data.map(modify_audio_path)\n",
    "\n",
    "# 打印修改后的第一个样本\n",
    "print(modified_data['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79df7b5a-ea16-4624-a452-171b0317f61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r-----+ 1 zhang.jinda1 van-speech-nlp 539K Feb 24 13:52 europarl_bilingual_kenlm_3-gram/output_model.klm_trigram.arpa\n"
     ]
    }
   ],
   "source": [
    "# 查看文件大小\n",
    "!ls -lh europarl_bilingual_kenlm_3-gram/output_model.klm_trigram.arpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "417c84fb-25e5-4986-85ff-8064bd0c9cb8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:58\u001b[0m\n\u001b[1;32m     56\u001b[0m   sys\u001b[38;5;241m.\u001b[39msetdlopenflags(_default_dlopen_flags \u001b[38;5;241m|\u001b[39m ctypes\u001b[38;5;241m.\u001b[39mRTLD_LOCAL)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpywrap_tensorflow_internal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:28\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _mod\n\u001b[0;32m---> 28\u001b[0m _pywrap_tensorflow_internal \u001b[38;5;241m=\u001b[39m \u001b[43mswig_import_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m swig_import_helper\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:24\u001b[0m, in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 24\u001b[0m     _mod \u001b[38;5;241m=\u001b[39m \u001b[43mimp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_pywrap_tensorflow_internal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpathname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdescription\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py:243\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_dynamic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m type_ \u001b[38;5;241m==\u001b[39m PKG_DIRECTORY:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py:343\u001b[0m, in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m spec \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mmachinery\u001b[38;5;241m.\u001b[39mModuleSpec(\n\u001b[1;32m    342\u001b[0m     name\u001b[38;5;241m=\u001b[39mname, loader\u001b[38;5;241m=\u001b[39mloader, origin\u001b[38;5;241m=\u001b[39mpath)\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mImportError\u001b[0m: /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m      3\u001b[0m dataset \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf-internal-testing/librispeech_asr_demo\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclean\u001b[39m\u001b[38;5;124m\"\u001b[39m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m dataset\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/datasets/__init__.py:43\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m pyarrow\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[0;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BeamBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/datasets/arrow_dataset.py:60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpa\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompute\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpc\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HfApi, HfFolder\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocess\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/__init__.py:63\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub_mixin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelHubMixin, PyTorchModelHubMixin\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m InferenceApi\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras_mixin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     64\u001b[0m     KerasModelHubMixin,\n\u001b[1;32m     65\u001b[0m     from_pretrained_keras,\n\u001b[1;32m     66\u001b[0m     push_to_hub_keras,\n\u001b[1;32m     67\u001b[0m     save_pretrained_keras,\n\u001b[1;32m     68\u001b[0m )\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrepository\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Repository\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msnapshot_download\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m snapshot_download\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/keras_mixin.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[0;32m---> 24\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_extract_hyperparameters_from_keras\u001b[39m(model):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/__init__.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mv1\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m app\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/__init__.py:49\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# TODO(drpng): write up instructions for editing this file in a doc and point to\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# the doc instead.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# If you want to edit this file to expose modules in public tensorflow API, you\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# go/tf-wildcard-import\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Protocol buffers\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_pb2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py:74\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m   msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124mSee https://www.tensorflow.org/install/errors\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124mfor some common reasons and solutions.  Include the entire stack trace\u001b[39m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124mabove this error message when asking for help.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m \u001b[38;5;241m%\u001b[39m traceback\u001b[38;5;241m.\u001b[39mformat_exc()\n\u001b[0;32m---> 74\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n",
      "\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py\", line 243, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/imp.py\", line 343, in load_dynamic\n    return _load(spec)\nImportError: /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so: invalid ELF header\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b40476-e805-4d44-a548-ff573f8a6885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "audio_sample = dataset[2]\n",
    "print(audio_sample[\"text\"].lower())\n",
    "ipd.Audio(data=audio_sample[\"audio\"][\"array\"], autoplay=True, rate=audio_sample[\"audio\"][\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c945e364-dd04-4934-be27-47b1aa5d3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c527b0a25c1d472995b0653e8a51c139",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cessor_config.json\";:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43874745cd924a9f9d9d86a8217d0ef9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)enizer_config.json\";:   0%|          | 0.00/376 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e54bda11db6471a85f55873873a471b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"vocab.json\";:   0%|          | 0.00/358 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "835cac3c3e77480083ed98fd632f49f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)al_tokens_map.json\";:   0%|          | 0.00/96.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dc4e51f219c4a38af97e1d8c60dd3b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"config.json\";:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4da83468d0d47d7a2a93b9b05752ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-100h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.mask_time_emb_vector']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-100h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-100h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-100h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b743a4e0-f147-4afd-bbcb-475af4dcb5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(audio_sample[\"audio\"][\"array\"], sampling_rate=16_000, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e781986-d4da-48b0-8eca-f5631b20aee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dbb2458-9633-458d-9faa-0c8d0757922c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he tells us that at this festive season of the year with christmaus and rose beef looming before us simalyis drawn from eating and its results occur most readily to the mind'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "transcription[0].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2525ba3e-d1b0-4ce7-8555-911832353965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (0.5.1)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (2024.2.0)\n",
      "Requirement already satisfied: requests in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (4.5.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface_hub) (23.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests->huggingface_hub) (2022.12.7)\n",
      "Using cached huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "  Attempting uninstall: huggingface_hub\n",
      "    Found existing installation: huggingface-hub 0.5.1\n",
      "    Uninstalling huggingface-hub-0.5.1:\n",
      "      Successfully uninstalled huggingface-hub-0.5.1\n",
      "Successfully installed huggingface_hub-0.20.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de38151b-3eaa-4bc2-be0f-e93924c72581",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "snapshot_download() got an unexpected keyword argument 'allow_regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Wav2Vec2ProcessorWithLM\n\u001b[0;32m----> 3\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mWav2Vec2ProcessorWithLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatrickvonplaten/wav2vec2-base-100h-with-lm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py:158\u001b[0m, in \u001b[0;36mWav2Vec2ProcessorWithLM.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     alphabet_filename \u001b[38;5;241m=\u001b[39m BeamSearchDecoderCTC\u001b[38;5;241m.\u001b[39m_ALPHABET_SERIALIZED_FILENAME\n\u001b[1;32m    156\u001b[0m     allow_regex \u001b[38;5;241m=\u001b[39m [language_model_filenames, alphabet_filename]\n\u001b[0;32m--> 158\u001b[0m     decoder \u001b[38;5;241m=\u001b[39m \u001b[43mBeamSearchDecoderCTC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_hf_hub\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_regex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# set language model attributes\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m attribute \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munk_score_offset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore_boundary\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/pyctcdecode/decoder.py:872\u001b[0m, in \u001b[0;36mBeamSearchDecoderCTC.load_from_hf_hub\u001b[0;34m(cls, model_id, cache_dir, **kwargs)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    868\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou need to install huggingface_hub to use `load_from_hf_hub`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee https://pypi.org/project/huggingface-hub/ for installation.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 872\u001b[0m cached_directory \u001b[38;5;241m=\u001b[39m \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=not-callable\u001b[39;49;00m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload_from_dir(cached_directory)\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    120\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(\n\u001b[1;32m    121\u001b[0m         fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[1;32m    122\u001b[0m     )\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: snapshot_download() got an unexpected keyword argument 'allow_regex'"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ProcessorWithLM\n",
    "\n",
    "processor = Wav2Vec2ProcessorWithLM.from_pretrained(\"patrickvonplaten/wav2vec2-base-100h-with-lm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caca22b0-453c-4f6e-a9e4-48fa651d5223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12.1\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import huggingface_hub; print(huggingface_hub.__version__)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35e95c-8301-41cb-bc49-854970899de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b444e9a-a499-424a-b9c0-eb312df35283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (2.10.0)\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.17.1-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: filelock in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (1.23.5)\n",
      "Collecting pyarrow>=12.0.0 (from datasets)\n",
      "  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: xxhash in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (0.20.3)\n",
      "Requirement already satisfied: packaging in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (3.0.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from huggingface-hub>=0.19.4->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: six>=1.5 in /work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
      "Downloading datasets-2.17.1-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.7/536.7 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyarrow, fsspec, datasets\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 11.0.0\n",
      "    Uninstalling pyarrow-11.0.0:\n",
      "      Successfully uninstalled pyarrow-11.0.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.2.0\n",
      "    Uninstalling fsspec-2024.2.0:\n",
      "      Successfully uninstalled fsspec-2024.2.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.10.0\n",
      "    Uninstalling datasets-2.10.0:\n",
      "      Successfully uninstalled datasets-2.10.0\n",
      "Successfully installed datasets-2.17.1 fsspec-2023.10.0 pyarrow-15.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a470537-bf3c-4369-b492-f0c925ab2205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fsspec==2023.9.2\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Downloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2023.10.0\n",
      "    Uninstalling fsspec-2023.10.0:\n",
      "      Successfully uninstalled fsspec-2023.10.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "s3fs 2023.10.0 requires fsspec==2023.10.0, but you have fsspec 2023.9.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed fsspec-2023.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fsspec==2023.9.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c3bbcf6-575c-415a-b046-b9b28d805452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘downloads’: File exists\n",
      "Error:\n",
      "\n",
      "\t[Errno 2] No such file or directory: '/content/downloads'\n",
      "\n",
      "To report issues, please visit https://github.com/wkentaro/gdown/issues.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'zipfile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m zip_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/downloads/Torgo.zip\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m extraction_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/downloads/Torgo/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241m.\u001b[39mZipFile(zip_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      9\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(extraction_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'zipfile' is not defined"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "# Download the Torgo dataset\n",
    "!mkdir downloads\n",
    "!gdown 1gnlQ0IAsza57tFOclFWfzOd4HxuwFn5U -O /content/downloads/Torgo.zip\n",
    "\n",
    "# Define the zip file and the extraction folder\n",
    "zip_file_path = '/content/downloads/Torgo.zip'\n",
    "extraction_path = '/content/downloads/Torgo/'\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extraction_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ac324d0-c738-4808-8d85-09035c4e4230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.855, (0.7938131968651883, 0.9126023142471228))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the main function \n",
    "from confidence_intervals import evaluate_with_conf_int\n",
    "\n",
    "# Define the metric of interest (could be a custom method)\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create a toy dataset for this example (to be replaced with your actual data)\n",
    "from confidence_intervals.utils import create_data\n",
    "decisions, labels, conditions = create_data(200, 200, 20)\n",
    "\n",
    "# Run the function. In this case, the samples are represented by the categorical decisions\n",
    "# made by the system which, along with the labels, is all that is needed to compute the metric.\n",
    "samples = decisions\n",
    "evaluate_with_conf_int(samples, accuracy_score, labels, conditions, num_bootstraps=1000, alpha=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b31fd28-c85c-4d6b-9fda-c37ce574b702",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'calculate_confidence_interval' from 'confidence_intervals' (/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/confidence_intervals/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfidence_intervals\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calculate_confidence_interval\n\u001b[1;32m      3\u001b[0m before_results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.9186046512\u001b[39m, \u001b[38;5;241m0.9069767442\u001b[39m]\n\u001b[1;32m      4\u001b[0m after_results \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.751572\u001b[39m, \u001b[38;5;241m0.704403\u001b[39m]\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'calculate_confidence_interval' from 'confidence_intervals' (/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/confidence_intervals/__init__.py)"
     ]
    }
   ],
   "source": [
    "from confidence_intervals import calculate_confidence_interval\n",
    "\n",
    "before_results = [0.9186046512, 0.9069767442]\n",
    "after_results = [0.751572, 0.704403]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c58500-17d7-45ec-bd80-681cf5c251ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci_before = calculate_confidence_interval(before_results)\n",
    "ci_after = calculate_confidence_interval(after_results)\n",
    "\n",
    "# 检查是否显著\n",
    "significant = ci_before[0] > ci_after[1] or ci_after[0] > ci_before[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c89b45db-8e16-4908-9cf3-2139633100ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05930a71-3274-404d-a0d5-fe6ad55af03e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/van-speech-nlp/jindaznb/asrenv'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a965fe51-e738-4b4f-92ba-2882d7c8f52e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                Size  Used Avail Use% Mounted on\n",
      "vast1-mghpcc-eth.neu.edu:/discovery/home  155T   80T   76T  52% /home\n"
     ]
    }
   ],
   "source": [
    "!df -h /home/zhang.jinda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e259034-3baa-455b-8162-094324898b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir()\n",
    "\n",
    "for file in files:\n",
    "    if file.startswith(\"wer\"):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccf8254c-11a1-4154-b832-3a9601aa98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "files = os.listdir()\n",
    "\n",
    "for file in files:\n",
    "    if file.startswith(\"413\"):\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1941e6ad-e511-4ab2-b41c-7bddaf1f7189",
   "metadata": {},
   "source": [
    "###### pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee3449d-3f7c-4e77-b523-7f1c2a033475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filesystem                                  Size  Used Avail Use% Mounted on\n",
      "devtmpfs                                     94G     0   94G   0% /dev\n",
      "tmpfs                                        94G  4.0K   94G   1% /dev/shm\n",
      "tmpfs                                        94G  108M   94G   1% /run\n",
      "tmpfs                                        94G     0   94G   0% /sys/fs/cgroup\n",
      "/dev/sda1                                    20G   18G  2.1G  90% /\n",
      "/dev/sda2                                    20G  1.2G   19G   6% /var\n",
      "/dev/sda4                                   389G   33M  389G   1% /tmp\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/scratch  2.8P  1.8P 1008T  65% /scratch\n",
      "vast1-mghpcc-ib.neu.edu:/courses             16T   12T  3.4T  79% /courses\n",
      "vast1-mghpcc-ib.neu.edu:/work_project       6.6P  4.4P  2.3P  66% /work\n",
      "vast1-mghpcc-ib.neu.edu:/discovery/home     155T   80T   76T  52% /home\n",
      "vast1-mghpcc-ib.neu.edu:/vast_shared         30T   15T   16T  47% /shared\n",
      "192.168.4.70:/datasets                       21T   21T  256G  99% /datasets\n",
      "tmpfs                                        19G     0   19G   0% /run/user/0\n"
     ]
    }
   ],
   "source": [
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17439cec-09c8-4d89-8f8b-41bdec09d2d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d70fa52d-ffd3-4dff-a5ba-66d39ac780c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libri.apra\n"
     ]
    }
   ],
   "source": [
    "ls /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f8f886-457e-46fa-af77-58d2884abeff",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files/libri.arpa'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files/unigram.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m unigrams \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marpa_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m      7\u001b[0m     start_extraction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m file:\n",
      "File \u001b[0;32m/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files/libri.arpa'"
     ]
    }
   ],
   "source": [
    "arpa_file = f\"/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files/libri.arpa\"\n",
    "output_file = f\"/work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/libri_lm_arpa_files/unigram.txt\"\n",
    "\n",
    "unigrams = []\n",
    "\n",
    "with open(arpa_file, \"r\") as file:\n",
    "    start_extraction = False\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if start_extraction:\n",
    "            if not line:  \n",
    "                break\n",
    "            parts = line.split(\"\\t\")\n",
    "            unigram = parts[1]  \n",
    "            unigrams.append(unigram)\n",
    "        elif line.startswith(\"\\\\1-grams:\"):\n",
    "            start_extraction = True\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for unigram in unigrams:\n",
    "        file.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7ea36fd-6189-48b1-9dad-514783117e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          41322980       gpu sys/dash zhang.ji  R      32:00      1 d1005\n",
      "          41323238     short auto_eur zhang.ji PD       0:00      1 (Priority)\n"
     ]
    }
   ],
   "source": [
    "!squeue -u zhang.jinda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ee7ee03-2e9b-47ed-a845-c2d7ee95d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scancel 41317504"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12dada5e-e507-4421-aa32-762fe37f3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 41323238\n"
     ]
    }
   ],
   "source": [
    "!sbatch auto_euro.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1704ab-9c4e-404f-8e16-29460fed5c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 41323250\n"
     ]
    }
   ],
   "source": [
    "!sbatch auto_torgo.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98b902ef-c13b-4305-9f61-46c4f091c7d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 41323262\n"
     ]
    }
   ],
   "source": [
    "!sbatch auto_libri.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fcf945-f1f1-4a76-979b-13e9385af476",
   "metadata": {},
   "outputs": [],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0dbab73-4106-4b22-9f0d-5193fdf1c1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Overall Rate: 2.47%\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (0.005263157895, 1),\n",
    "    (2.21, 17),\n",
    "    (2.69, 18),\n",
    "    (2.48, 15),\n",
    "    (2.41, 15),\n",
    "    (2.33, 15),\n",
    "    (2.00, 9),\n",
    "    (2.92, 21)\n",
    "]\n",
    "\n",
    "# Calculate weighted contributions\n",
    "weighted_contributions = [percentage * count for percentage, count in data]\n",
    "\n",
    "# Calculate overall weighted rate\n",
    "overall_weighted_rate = sum(weighted_contributions) / sum(count for _, count in data)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted Overall Rate: {overall_weighted_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e11f5016-45f9-43ec-a7e8-61fee3c4678b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Overall Rate: 4.72%\n"
     ]
    }
   ],
   "source": [
    "# Provided data\n",
    "data = [\n",
    "    (5.70, 4),\n",
    "    (4.56, 35),\n",
    "    (4.63, 31),\n",
    "    (4.63, 28),\n",
    "    (4.66, 29),\n",
    "    (4.50, 29),\n",
    "    (5.11, 23),\n",
    "    (4.87, 35)\n",
    "]\n",
    "\n",
    "# Calculate weighted contributions\n",
    "weighted_contributions = [percentage * count for percentage, count in data]\n",
    "\n",
    "# Calculate overall weighted rate\n",
    "overall_weighted_rate = sum(weighted_contributions) / sum(count for _, count in data)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Weighted Overall Rate: {overall_weighted_rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da539e56-2ce5-41cd-97a4-d95be2bacc80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ac81c935-2cc7-473a-a6ef-a47a07ab9b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04adc954-b3c8-4569-a42f-6affd650916c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07616cfc-eddf-4947-a6d8-f9067c278857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2540dcc6-d116-47a5-a5f2-cec1e1050cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50346051-0727-45c5-846c-9dd5e0b73699",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
