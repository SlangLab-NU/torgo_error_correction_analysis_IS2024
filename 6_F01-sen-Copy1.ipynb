{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62834c84-8927-4023-b24c-67e5db655b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# User input:\n",
    "test_speaker = \"F01\"\n",
    "split=\"val\"\n",
    "level = 'sentence'\n",
    "pattern = \"keep_all\"\n",
    "ngram_order=3\n",
    "\n",
    "text_count_threshold = 40\n",
    "model_user = \"macarious\"\n",
    "model_repo = f\"torgo_xlsr_finetune_{test_speaker}\"\n",
    "model_repo_path = f\"{model_user}/{model_repo}\"\n",
    "\n",
    "kenlm_model_user = \"macarious\"\n",
    "kenlm_model_repo = f\"europarl_bilingual_kenlm_{ngram_order}-gram\"\n",
    "kenlm_model_repo_path= f\"{kenlm_model_user}/{kenlm_model_repo}\"\n",
    "if ngram_order == 1:\n",
    "  kenlm_model = \"\"\n",
    "else:\n",
    "  kenlm_model = f\"{ngram_order}gram.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f53a5bd-31b9-4344-9d15-dbf50b95eb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 30_eval_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95301722-8580-4dff-aafa-971d8d0e75e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n",
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/google/colab/data_table.py:30: UserWarning: IPython.utils.traitlets has moved to a top-level traitlets package.\n",
      "  from IPython.utils import traitlets as _traitlets\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "\n",
    "from huggingface_hub import Repository\n",
    "from datasets import load_dataset, DatasetDict, Dataset, Audio\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC, Wav2Vec2ProcessorWithLM\n",
    "from pyctcdecode import build_ctcdecoder\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from tqdm import tqdm\n",
    "from evaluate import load\n",
    "from datetime import datetime\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53dc4366-b87d-4c6c-b31a-15d43b621e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Download the trained model\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_repo_path)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91a6efd7-be1a-4e10-ada8-ac29877f2057",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_local_path = f\"kenlm_model_{ngram_order}gram_words_europarl\"\n",
    "# lm_repo = Repository(local_dir=lm_local_path, clone_from=kenlm_model_repo_path)\n",
    "# lm_repo.git_pull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11d4d5f5-d61d-488d-8e78-a81ef67ad86a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo.csv')\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b05a87-5299-44c8-9b91-9537c9add497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['session', 'audio', 'text', 'speaker_id'],\n",
       "        num_rows: 15091\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['session', 'audio', 'text', 'speaker_id'],\n",
       "        num_rows: 1075\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['session', 'audio', 'text', 'speaker_id'],\n",
       "        num_rows: 228\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split data into train, valid, test sets\n",
    "valid_speaker = 'F03' if test_speaker != 'F03' else 'F04'\n",
    "train_speaker = [s for s in speakers if s not in [test_speaker, valid_speaker]]\n",
    "\n",
    "torgo_dataset = DatasetDict()\n",
    "torgo_dataset['train'] = dataset_csv['train'].filter(lambda x: x in train_speaker, input_columns=['speaker_id'])\n",
    "torgo_dataset['validation'] = dataset_csv['train'].filter(lambda x: x == valid_speaker, input_columns=['speaker_id'])\n",
    "torgo_dataset['test'] = dataset_csv['train'].filter(lambda x: x == test_speaker, input_columns=['speaker_id'])\n",
    "\n",
    "torgo_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d27846d-fe48-41a2-9cc9-4e1a7391e2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of times the text has been spoken in each of the 'train',\n",
    "# 'validation', and 'test' sets. Remove text according to the\n",
    "# text_count_threshold from a previous cell.\n",
    "\n",
    "if pattern == \"no_keep\":\n",
    "    unique_texts = set(torgo_dataset['train'].unique(column='text')) | set(torgo_dataset['validation'].unique(column='text')) | set(torgo_dataset['test'].unique(column='text'))\n",
    "    unique_texts_count = {}\n",
    "    \n",
    "    for text in unique_texts:\n",
    "      unique_texts_count[text] = {'train_validation': 0, 'test': 0}\n",
    "    \n",
    "    for text in torgo_dataset['train']['text']:\n",
    "      unique_texts_count[text]['train_validation'] += 1\n",
    "    \n",
    "    for text in torgo_dataset['validation']['text']:\n",
    "      unique_texts_count[text]['train_validation'] += 1\n",
    "    \n",
    "    for text in torgo_dataset['test']['text']:\n",
    "      unique_texts_count[text]['test'] += 1\n",
    "    \n",
    "    texts_to_keep_in_train_validation = []\n",
    "    texts_to_keep_in_test = []\n",
    "    for text in unique_texts_count:\n",
    "      if unique_texts_count[text]['train_validation'] < text_count_threshold and unique_texts_count[text]['test'] > 0:\n",
    "        texts_to_keep_in_test.append(text)\n",
    "      else:\n",
    "        texts_to_keep_in_train_validation.append(text)\n",
    "    \n",
    "    original_data_count = {'train': len(torgo_dataset['train']), 'validation': len(torgo_dataset['validation']), 'test': len(torgo_dataset['test'])}\n",
    "    \n",
    "    # Update the three dataset splits\n",
    "    torgo_dataset['train'] = torgo_dataset['train'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "    torgo_dataset['validation'] = torgo_dataset['validation'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "    torgo_dataset['test'] = torgo_dataset['test'].filter(lambda x: x['text'] in texts_to_keep_in_test)\n",
    "    \n",
    "    print(f'Train:       {len(torgo_dataset[\"train\"])}/{original_data_count[\"train\"]} ({len(torgo_dataset[\"train\"]) * 100 // original_data_count[\"train\"]}%)')\n",
    "    print(f'Validation:  {len(torgo_dataset[\"validation\"])}/{original_data_count[\"validation\"]} ({len(torgo_dataset[\"validation\"]) * 100 // original_data_count[\"validation\"]}%)')\n",
    "    print(f'Test:        {len(torgo_dataset[\"test\"])}/{original_data_count[\"test\"]} ({len(torgo_dataset[\"test\"]) * 100 // original_data_count[\"test\"]}%)')\n",
    "    \n",
    "    print()\n",
    "    torgo_dataset\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "080b972f-4068-4bdc-9bab-1237f068bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to process data:\n",
    "\n",
    "# Remove special characters and convert all text into lowercase\n",
    "chars_to_ignore_regex = '[\\,\\?\\.\\!\\-\\;\\:\\\"0-9]'\n",
    "sampling_rate=16000\n",
    "\n",
    "def remove_special_characters(batch):\n",
    "    batch['text'] = re.sub(chars_to_ignore_regex, ' ', batch['text']).lower()\n",
    "    return batch\n",
    "\n",
    "def prepare_torgo_dataset(batch):\n",
    "    # Load audio data into batch\n",
    "    audio = batch['audio']\n",
    "\n",
    "    # Extract values\n",
    "    batch[\"input_values\"] = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_values[0]\n",
    "    batch[\"input_length\"] = len(batch[\"input_values\"])\n",
    "\n",
    "    # Encode to label ids\n",
    "    batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec366ed8-8336-4d49-bebc-d27a00f01cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_values', 'input_length', 'labels'],\n",
       "    num_rows: 267\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torgo_test_set = torgo_dataset['validation']\n",
    "'''\n",
    "  ******************** For debugging ********************\n",
    "'''\n",
    "# torgo_test_set = torgo_test_set.select(range(50))\n",
    "'''\n",
    "  ******************** For debugging ********************\n",
    "'''\n",
    "\n",
    "# Remove special characters\n",
    "torgo_test_set = torgo_test_set.map(remove_special_characters)\n",
    "\n",
    "if level == 'sentence':\n",
    "    torgo_test_set = torgo_test_set.filter(lambda example: len(example[\"text\"].split()) > 1)\n",
    "else:\n",
    "    torgo_test_set = torgo_test_set.filter(lambda example: len(example[\"text\"].split()) == 1)\n",
    "\n",
    "# Filter audio within a certain length\n",
    "torgo_test_set = torgo_test_set.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))\n",
    "torgo_test_set = torgo_test_set.map(\n",
    "  prepare_torgo_dataset,\n",
    "  remove_columns=['session', 'audio', 'speaker_id'],\n",
    "  num_proc=4)\n",
    "\n",
    "min_input_length_in_sec = 1.0\n",
    "max_input_length_in_sec = 10.0\n",
    "torgo_test_set = torgo_test_set.filter(lambda x: x < max_input_length_in_sec * sampling_rate, input_columns=[\"input_length\"])\n",
    "torgo_test_set = torgo_test_set.filter(lambda x: x > min_input_length_in_sec * sampling_rate, input_columns=[\"input_length\"])\n",
    "\n",
    "print()\n",
    "torgo_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a27d9277-a704-472b-98b2-d55e302fc40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel(processor, model, dataset, lm_model_path=None):\n",
    "\n",
    "  predictions = []\n",
    "  references = []\n",
    "\n",
    "  if not lm_model_path:\n",
    "    for i in tqdm(range(dataset.num_rows)):\n",
    "      inputs = processor(dataset[i][\"input_values\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "      with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "      predicted_ids = torch.argmax(logits, dim=-1)\n",
    "      transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "      predictions.append(transcription[0].lower())\n",
    "      references.append(dataset[i][\"text\"])\n",
    "\n",
    "  else:\n",
    "    vocab_dict = processor.tokenizer.get_vocab()\n",
    "    sorted_vocab_dict = {k: v for k, v in sorted(\n",
    "        vocab_dict.items(), key=lambda item: item[1])}\n",
    "\n",
    "    unigrams = set()\n",
    "\n",
    "    with open(f\"{lm_local_path}/unigrams.txt\", \"r\") as f:\n",
    "      for line in f:\n",
    "        line = line.strip()\n",
    "        unigrams.add(line)\n",
    "\n",
    "    # Implement language model in the decoder\n",
    "    decoder = build_ctcdecoder(\n",
    "        labels=list(sorted_vocab_dict.keys()),\n",
    "        kenlm_model_path=lm_model_path if ngram_order > 1 else None,\n",
    "        unigrams=unigrams\n",
    "    )\n",
    "\n",
    "    # Build new processor with new decoder\n",
    "    processor = Wav2Vec2ProcessorWithLM(\n",
    "        feature_extractor=processor.feature_extractor,\n",
    "        tokenizer=processor.tokenizer,\n",
    "        decoder=decoder\n",
    "    )\n",
    "\n",
    "    # Transcripe the audio\n",
    "    for i in tqdm(range(dataset.num_rows)):\n",
    "      inputs = processor(dataset[i][\"input_values\"], sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "      with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "\n",
    "      transcription = processor.batch_decode(logits.numpy()).text\n",
    "\n",
    "      predictions.append(transcription[0].lower())\n",
    "      references.append(dataset[i][\"text\"])\n",
    "\n",
    "  # Calculate the wer score\n",
    "  wer = load(\"wer\")\n",
    "  wer_score = wer.compute(predictions=predictions, references=references)\n",
    "\n",
    "  return wer_score, predictions, references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1346d576-3ec1-4089-b4b6-a33a4dcc5570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 38/267 [2:01:59<11:58:19, 188.21s/it]"
     ]
    }
   ],
   "source": [
    "wer_score_no_lm, predictions_no_lm, references_no_lm = evaluateModel(processor, model, torgo_test_set)\n",
    "\n",
    "print(f\"WER (no LM): {wer_score_no_lm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b135cbbe-9132-4d8d-afb7-14c45be12c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_score_lm, predictions_lm, references_lm = evaluateModel(processor, model, torgo_test_set, f\"{lm_local_path}/{kenlm_model}\")\n",
    "\n",
    "print(f\"WER ({ngram_order}-gram): {wer_score_lm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f535880-eae6-4c50-a978-aa3622c22ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = set()\n",
    "\n",
    "with open(f\"{lm_local_path}/unigrams.txt\", \"r\") as f:\n",
    "  for line in f:\n",
    "    line = line.strip()\n",
    "    unigrams.add(line)\n",
    "\n",
    "print(len(set(\"\".join(unigrams))))\n",
    "print(set(\"\".join(unigrams)))\n",
    "print(unigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53a8d4-4308-4290-88a8-9da46ad91475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Save results to a csv file\n",
    "with open(f\"results_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\", \"w\") as csv_file:\n",
    "  csv_writer = csv.writer(csv_file)\n",
    "  csv_writer.writerow([\"Prediction (no LM)\", f\"Prediction ({ngram_order}-gram)\", \"Reference\"])\n",
    "  for i in range(len(predictions_no_lm)):\n",
    "    csv_writer.writerow([predictions_no_lm[i], predictions_lm[i], references_lm[i]])\n",
    "\n",
    "# Display as dataframe\n",
    "results_df = pd.read_csv(f\"results_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\")\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2104de4-e6d9-4e7b-84cf-51725becba49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save wer to a csv file\n",
    "\n",
    "with open(f\"wer_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\", \"w\") as csv_file:\n",
    "  csv_writer = csv.writer(csv_file)\n",
    "  csv_writer.writerow([\"Language Model\", \"WER\"])\n",
    "  csv_writer.writerow([\"None\", wer_score_no_lm])\n",
    "  csv_writer.writerow([f\"{ngram_order}-gram\", wer_score_lm])\n",
    "\n",
    "# Display as dataframe\n",
    "results_wer_df = pd.read_csv(f\"wer_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\")\n",
    "results_wer_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9aca74-3dea-4037-b0eb-76d09643c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a string of current date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Zip the results into a single file for download\n",
    "output_zip_path = f\"results_with_LM_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}_{current_date}.zip\"\n",
    "with zipfile.ZipFile(output_zip_path, \"w\") as zip_file:\n",
    "  zip_file.write(f\"results_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\")\n",
    "  zip_file.write(f\"wer_{ngram_order}gram_{test_speaker}_{test_speaker}_{level}_{pattern}_{split}.txt\")\n",
    "\n",
    "files.download(output_zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875b2e59-7340-4757-9bb2-6c48afd6b58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
