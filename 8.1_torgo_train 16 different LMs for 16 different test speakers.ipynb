{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83a11ac7-d254-4d40-b22f-fe3069c6deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/van-speech-nlp/jindaznb/asrenv/lib/python3.10/site-packages/huggingface_hub/utils/_runtime.py:184: UserWarning: Pydantic is installed but cannot be imported. Please check your installation. `huggingface_hub` will default to not using Pydantic. Error message: '{e}'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%run 10_ngram_common.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdf65a7-30c0-44de-8817-f179048b1661",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -U datasets\n",
    "# !pip install fsspec==2023.9.2\n",
    "# pattern = \"no_keep\"\n",
    "pattern = \"keep_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51c513c7-12f9-4e83-a0e3-b619f10606db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speakers: F01, F03, F04, FC01, FC02, FC03, M01, M02, M03, M04, M05, MC01, MC02, MC03, MC04\n"
     ]
    }
   ],
   "source": [
    "# Read the dataset\n",
    "data_df = pd.read_csv('torgo.csv')\n",
    "dataset_csv = load_dataset('csv', data_files='torgo.csv',cache_dir=None)\n",
    "\n",
    "speakers = data_df['speaker_id'].unique()\n",
    "\n",
    "print(f'Speakers: {\", \".join(speakers)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f10b482f-53e3-4b34-bc12-b082f36766e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F01: 15091\n",
      "F03: 14652\n",
      "F04: 14652\n",
      "M01: 14580\n",
      "M02: 14553\n",
      "M03: 14519\n",
      "M04: 14667\n",
      "M05: 14746\n"
     ]
    }
   ],
   "source": [
    "atypical_speaker_texts = {}\n",
    "\n",
    "for speaker_id in atypical_speakers:\n",
    "    test_speaker=speaker_id\n",
    "    valid_speaker = 'F03' if test_speaker != 'F03' else 'F04'\n",
    "    train_speaker = [s for s in speakers if s not in [test_speaker, valid_speaker]]\n",
    "    \n",
    "    torgo_dataset = DatasetDict()\n",
    "    torgo_dataset['train'] = dataset_csv['train'].filter(lambda x: x in train_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['validation'] = dataset_csv['train'].filter(lambda x: x == valid_speaker, input_columns=['speaker_id'])\n",
    "    torgo_dataset['test'] = dataset_csv['train'].filter(lambda x: x == test_speaker, input_columns=['speaker_id'])\n",
    "\n",
    "    if pattern == \"no_keep\":\n",
    "        unique_texts = set(torgo_dataset['train'].unique(column='text')) | set(torgo_dataset['validation'].unique(column='text')) | set(torgo_dataset['test'].unique(column='text'))\n",
    "        unique_texts_count = {}\n",
    "        \n",
    "        for text in unique_texts:\n",
    "          unique_texts_count[text] = {'train_validation': 0, 'test': 0}\n",
    "        \n",
    "        for text in torgo_dataset['train']['text']:\n",
    "          unique_texts_count[text]['train_validation'] += 1\n",
    "        \n",
    "        for text in torgo_dataset['validation']['text']:\n",
    "          unique_texts_count[text]['train_validation'] += 1\n",
    "        \n",
    "        for text in torgo_dataset['test']['text']:\n",
    "          unique_texts_count[text]['test'] += 1\n",
    "        \n",
    "        texts_to_keep_in_train_validation = []\n",
    "        texts_to_keep_in_test = []\n",
    "        for text in unique_texts_count:\n",
    "          if unique_texts_count[text]['train_validation'] < text_count_threshold and unique_texts_count[text]['test'] > 0:\n",
    "            texts_to_keep_in_test.append(text)\n",
    "          else:\n",
    "            texts_to_keep_in_train_validation.append(text)\n",
    "        \n",
    "        original_data_count = {'train': len(torgo_dataset['train']), 'validation': len(torgo_dataset['validation']), 'test': len(torgo_dataset['test'])}\n",
    "        \n",
    "        # Update the three dataset splits\n",
    "        torgo_dataset['train'] = torgo_dataset['train'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "        torgo_dataset['validation'] = torgo_dataset['validation'].filter(lambda x: x['text'] in texts_to_keep_in_train_validation)\n",
    "        torgo_dataset['test'] = torgo_dataset['test'].filter(lambda x: x['text'] in texts_to_keep_in_test)\n",
    "        \n",
    "        print(f'Train:       {len(torgo_dataset[\"train\"])}/{original_data_count[\"train\"]} ({len(torgo_dataset[\"train\"]) * 100 // original_data_count[\"train\"]}%)')\n",
    "        print(f'Validation:  {len(torgo_dataset[\"validation\"])}/{original_data_count[\"validation\"]} ({len(torgo_dataset[\"validation\"]) * 100 // original_data_count[\"validation\"]}%)')\n",
    "        print(f'Test:        {len(torgo_dataset[\"test\"])}/{original_data_count[\"test\"]} ({len(torgo_dataset[\"test\"]) * 100 // original_data_count[\"test\"]}%)')\n",
    "        \n",
    "        print()\n",
    "        torgo_dataset\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    texts = torgo_dataset['train']['text']\n",
    "    \n",
    "    cleaned_texts = []\n",
    "    for text in texts:\n",
    "        cleaned_text = ' '.join(re.sub(r'[^a-zA-Z0-9]', '', word.lower()) for word in text.split())\n",
    "        cleaned_texts.append(cleaned_text)\n",
    "        \n",
    "    atypical_speaker_texts[speaker_id] = cleaned_texts\n",
    "\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    print(f\"{speaker_id}: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e6735f-0937-4eba-9d41-d433f0ec1b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"torgo_lm_arpa_files\", exist_ok=True)\n",
    "\n",
    "for speaker_id, texts in atypical_speaker_texts.items():\n",
    "    with open(f\"torgo_lm_arpa_files/{speaker_id}_texts_{pattern}.txt\", \"w\") as file:\n",
    "        for text in texts:\n",
    "            file.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66e1d06d-a8b7-4e79-a754-dee17cad7fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F01_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 38277 types 1564\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18768 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1564 D1=0.801383 D2=1.16311 D3+=1.89263\n",
      "2 3306 D1=0.5 D2=1 D3+=1.5\n",
      "3 2743 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18768 2:52896 3:54860\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18768 2:52896 3:54860\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19800004 kB\tVmRSS:3872 kB\tRSSMax:4536752 kB\tuser:0.422249\tsys:2.64929\tCPU:3.07156\treal:3.09143\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F03_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 37133 types 1560\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18720 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1560 D1=0.801008 D2=1.19392 D3+=1.85139\n",
      "2 3299 D1=0.5 D2=1 D3+=1.5\n",
      "3 2738 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54760\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54760\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19797948 kB\tVmRSS:3868 kB\tRSSMax:4536748 kB\tuser:0.473972\tsys:2.54548\tCPU:3.0195\treal:3.04851\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/F04_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 37133 types 1560\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18720 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1560 D1=0.801008 D2=1.19392 D3+=1.85139\n",
      "2 3299 D1=0.5 D2=1 D3+=1.5\n",
      "3 2738 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54760\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54760\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19800000 kB\tVmRSS:3868 kB\tRSSMax:4536816 kB\tuser:0.405004\tsys:2.56669\tCPU:2.97176\treal:2.98287\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M01_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 36984 types 1562\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18744 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1562 D1=0.801133 D2=1.17859 D3+=1.87248\n",
      "2 3302 D1=0.5 D2=1 D3+=1.5\n",
      "3 2741 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18744 2:52832 3:54820\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18744 2:52832 3:54820\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19797948 kB\tVmRSS:3876 kB\tRSSMax:4536804 kB\tuser:0.435061\tsys:2.49662\tCPU:2.9317\treal:2.96406\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M02_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 36941 types 1560\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18720 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1560 D1=0.800882 D2=1.17884 D3+=1.87283\n",
      "2 3299 D1=0.5 D2=1 D3+=1.5\n",
      "3 2741 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54820\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18720 2:52784 3:54820\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19800000 kB\tVmRSS:3880 kB\tRSSMax:4536780 kB\tuser:0.421929\tsys:2.50864\tCPU:2.93059\treal:2.93737\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M03_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 36817 types 1564\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18768 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1564 D1=0.801383 D2=1.17833 D3+=1.87213\n",
      "2 3306 D1=0.5 D2=1 D3+=1.5\n",
      "3 2745 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18768 2:52896 3:54900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18768 2:52896 3:54900\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19797944 kB\tVmRSS:3872 kB\tRSSMax:4536756 kB\tuser:0.450424\tsys:2.40692\tCPU:2.85737\treal:2.88507\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M04_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 37179 types 1562\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18744 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1562 D1=0.802391 D2=1.18739 D3+=1.8494\n",
      "2 3298 D1=0.5 D2=1 D3+=1.5\n",
      "3 2737 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 165 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18744 2:52768 3:54740\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18744 2:52768 3:54740\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19795896 kB\tVmRSS:3872 kB\tRSSMax:4536776 kB\tuser:0.45494\tsys:2.34454\tCPU:2.79952\treal:2.81677\n",
      "=== 1/5 Counting and sorting n-grams ===\n",
      "Reading /work/van-speech-nlp/jindaznb/jslpnb/torgo_error_correction/torgo_lm_arpa_files/M05_texts_keep_all.txt\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Unigram tokens 37416 types 1560\n",
      "=== 2/5 Calculating and sorting adjusted counts ===\n",
      "Chain sizes: 1:18720 2:6997407744 3:13120140288\n",
      "Substituting fallback discounts for order 1: D1=0.5 D2=1 D3+=1.5\n",
      "Substituting fallback discounts for order 2: D1=0.5 D2=1 D3+=1.5\n",
      "Statistics:\n",
      "1 1560 D1=0.801008 D2=1.17871 D3+=1.93199\n",
      "2 3297 D1=0.5 D2=1 D3+=1.5\n",
      "3 2734 D1=0.5 D2=1 D3+=1.5\n",
      "Memory estimate for binary LM:\n",
      "type     kB\n",
      "probing 164 assuming -p 1.5\n",
      "probing 190 assuming -r models -p 1.5\n",
      "trie     85 without quantization\n",
      "trie     61 assuming -q 8 -b 8 quantization \n",
      "trie     83 assuming -a 22 array pointer compression\n",
      "trie     59 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
      "=== 3/5 Calculating and sorting initial probabilities ===\n",
      "Chain sizes: 1:18720 2:52752 3:54680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
      "Chain sizes: 1:18720 2:52752 3:54680\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "####################################################################################################\n",
      "=== 5/5 Writing ARPA model ===\n",
      "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
      "****************************************************************************************************\n",
      "Name:lmplz\tVmPeak:19800000 kB\tVmRSS:3872 kB\tRSSMax:4536776 kB\tuser:0.454592\tsys:2.49276\tCPU:2.94738\treal:2.96374\n"
     ]
    }
   ],
   "source": [
    "arpa_dir = \"torgo_lm_arpa_files\"\n",
    "os.makedirs(arpa_dir, exist_ok=True)\n",
    "\n",
    "for file_name in os.listdir(\"torgo_lm_arpa_files\"):\n",
    "    if file_name.endswith(f\"{pattern}.txt\"):\n",
    "        speaker_id = file_name.split(\"_\")[0]\n",
    "        arpa_file = f\"{arpa_dir}/{speaker_id}_3gram_{pattern}.arpa\"\n",
    "        txt_file = f\"torgo_lm_arpa_files/{file_name}\"\n",
    "        !kenlm/build/bin/lmplz -o 3 < \"{txt_file}\" > \"{arpa_file}\" -S 10% --discount_fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3189db50-781e-46e3-be87-01294ff1d86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "atypical_speakers = ['F01', 'F03', 'F04', 'M01', 'M02', 'M03', 'M04', 'M05']\n",
    "\n",
    "for speaker in atypical_speakers:\n",
    "    arpa_file = f\"torgo_lm_arpa_files/{speaker}_3gram_{pattern}.arpa\"\n",
    "    output_file = f\"torgo_lm_arpa_files/{speaker}_unigram_{pattern}.txt\"\n",
    "\n",
    "    unigrams = []\n",
    "\n",
    "    with open(arpa_file, \"r\") as file:\n",
    "        start_extraction = False\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if start_extraction:\n",
    "                if not line:  \n",
    "                    break\n",
    "                parts = line.split(\"\\t\")\n",
    "                unigram = parts[1]  \n",
    "                unigrams.append(unigram)\n",
    "            elif line.startswith(\"\\\\1-grams:\"):\n",
    "                start_extraction = True\n",
    "\n",
    "    with open(output_file, \"w\") as file:\n",
    "        for unigram in unigrams:\n",
    "            file.write(unigram + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76f428d-6e04-48dc-9892-225df8f573a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
