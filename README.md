Automatic speech recognition (ASR) for dysarthric speakers remains challenging. In developing real-world systems, often there is little or no access to the prompts said by the original speaker. Our study compares the role of post-processing with language models and sequence-to-sequence (seq2seq) error-correction systems trained on out-of-domain text.

The TORGO dataset is used to study the impact of modeling choices for dysarthric ASR. There is a high-degree of prompt-overlap between speakers in this dataset. Our study contributes a protocol to stratify utterances to prevent prompt-overlap between speakers. Strong baseline results using cross-lingual representations from the wav2vec2-xlsr model are presented on isolated words as well as sentences. After presenting the impact of out-of-domain n-gram language models, our study looks at the impact of a dual-encoder sequence-to-sequence model for error-correction. The dual-encoder model consumes the erroneous string after ASR along with its phonetic expansion. Our results suggest that phoneme level inputs are useful especially to correct errors at the word and sentence-level.
